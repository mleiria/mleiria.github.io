---
layout: post
title:  Álgebra Linear. O essencial.
date:   2020-06-04 16:50:00
---

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<h2>Multiplicação de Matrizes e Vectores</h2>
<p>Uma das mais importantes operações que envolvem matrizes é a multiplicação de duas matrizes.</p>
O <b>produto matricial</b> entre duas matrizes \(A\) e \(B\) é uma terceira matriz \(C\).<br>
Para este produto ser definido, a matriz \(A\) tem de ter o mesmo número de colunas que o número de linhas
da matriz \(B\).<br>
Se \(A\) tiver dimensões (\(m\) x \(n\)), onde \(m\) representa o número de linhas e \(m\) é o número de colunas e \(B\) tiver dimensões 
\((n\) x \(p\)), então \(C\) terá dimensões (\(m\) x \(p\)).<br>
Por exemplo:<p>
	
	\[C = AB\]
</p>
Esta operação é definida da seguinte forma:<p>

	\[C_{i,j} = \sum_{k} A_{i,k} B_{k,j}\]
</p>
Atenção que este produto não corresponde ao produto dos elementos individuais. Este produto também existe e chama-se <b>produto Hadamard</b> ou 
<b>produto elemento a elemento </b>(<i>element-wise</i>) e denota-se 
por \(A \odot B\)
<p>
O <b>produto vectorial</b> (<i>dot product</i>) entre dois vectores \(x\) e \(y\) com a mesma dimensão, é o produto matricial \(x^T y\)<br>
O produto matricial apresenta algumas propriedades úteis. Por exemplo a distributividade:<p>
	\[A(B + C) = AB + AC\]
</p>Também é associativo:<p>
	\[A(BC) = (AB)C\]
</p>
A multiplicação matricial <i>não é comutativa</i> (a condição \(AB = BA\) nem sempre é válida). No entanto o produto vectorial entre dois
vectores é comutativa:<p>
	\[x^Ty = y^Tx\]
</p>
A matriz transposta do produto de duas matrizes é:<p>
	\[(AB)^T = B^TA^T\]
</p>
<h2>Identidade e Matriz Inversa</h2>
Para descrever a matriz inversa, precisamos primeiro definir o conceito de <b>matriz identidade</b>. Uma matriz identidade é uma matriz que
não altera qualquer vector que for multiplicado por ela. Formalmente, \(I_{n} \in R^{nxn}\) e <p>
	\[\forall x \in R^{n}, I_nx = x\]
</p>
A estrutura da matriz identidade é simples: todas as entradas da diagonal principal são 1 e todas as outras são 0. Por exemplo:<p>
	\[I_3 = \begin{bmatrix}1 & 0 & 0\\0 & 1 & 0 \\0 & 0& 1\end{bmatrix}\]
</p>
A <b>matriz inversa</b> de \(A\), denota-se por \(A^{-1}\) e é difinida como a matriz tal que:<p>
	\[A^{-1} A =I_n\]
</p>
Consideremos como exercício final o seguinte sistema de equações lineares:<p>
	\[A x = b\]
</p>
onde \(A \in R^{mxn}\) é uma matriz com os valores conhecidos, \(b \in R^m\) é um vector conhecido e \(x \in R^{n}\) é o vector de variáveis 
desconhecidas para o qual pretendemos resolver o sistema. Esta equação resolve-se seguindo os seguintes passos:<p>
	\[A x = b\]
	\[A^{-1}A x = A^{-1}b\]
	\[I_n x = A^{-1}b\]
	\[x = A^{-1}b\]
</p>
<h2>Norma</h2>
Em <i>machine learning</i> normalmente mede-se o comprimento de um vector usando uma função chamada <b>norma</b>. Formalmente,
a norma \(L^p\) é dada por:<p>
	\[||x||_p = (\sum_i |x_i|^p)^{1/p}\]
para \(p \in R, p \geq 1\)
</p>
As normas são funções que mapeiam vectores para valores não negativos. Intiuitivamente podemos pensar que a norma de um vector \(x\) 
mede a distância da origem ao ponto \(x\). Sendo um pouco mais rigorosos, a norma é qualquer função \(f\) que satisfaz as seguintes propriedades:<p>
	<ul>
		<li>\(f(x) = 0\ \Rightarrow x = 0\)</li>
		<li>\(f(x + y) \leq f(x) + f(y)\)</li>
		<li>\(\forall \alpha \in R, f(\alpha x) = |\alpha|f(x)\)</li>
	</ul>
</p>
A norma \(L^2\) (quando \(p = 2\)) é conhecida como a <b>norma Euclideana</b>, que é simplesmente a distância euclideana da origem ao ponto 
identificado por \(x\). Costuma-se designar por \(||x||\), onde o \(2\) é omitido na notação.<br>
Também é bastante comum medir o comprimento de um vector usando o quadrado da norma \(L^2\), que pode ser calculado simplesmente como
\(x^Tx\)<br>
A norma \(L^1\) também é bastante utilizada:<p>
	\[||x||_1 = \sum_i |x_i|\]
</p>
Outra norma bastante utilizada nesta área é a norma \(L^\infty\), também conhecida como <b>norma máxima</b> (<i>max norm</i>):<p>
	\[||x||_\infty = \mathrm{max}_i|x_i|\]
</p>
Por fim, muitas vezes é necessário calcular o comprimento de uma matriz. No contexto de <i>deep learning</i> a norma mais utilizada 
é a <b>norma Frobenius</b>:<p>
	\[||A||_F = \sqrt{\sum_{i,j} A^2_{i,j}}\]
</p>
que é semelhante à norma \(L^2\) de um vector.<br>
	O produto vectorial de dois vectores pode ser escrito em termos das suas normas:<p>
		\[x^Ty = ||x||_2 ||y||_2 cos \theta\]
</p>
onde \(\theta\) é o ângulo entre \(x\) e \(y\)
<h2>Matrizes e vectores especiais</h2>
<h3>Matriz diagonal</h3>
Esta matriz só apresenta valores diferentes de zero na sua diagonal principal. Formalmente, uma matriz \(D\) é diagonal se e só se \(D_{i,j} = 0\) para todos
os \(i \neq j\). Já vimos um exemplo de uma matriz diagonal: a matriz identidade, onde todas as entradas na diagonal principal são 1.
<h3>Matriz simétrica</h3>
É qualquer matriz tal que é igual à sua transposta:<p>
	\[A = A^T\]
</p>
<h3>Vector unitário</h3>
É um vector com a <b>norma unitária</b>:<p>
	\[||x||_2 = 1\]
</p>
Um vector \(x\) e um vector \(y\) são ortogonais entre si, se \(x^Ty = 0\). Se ambos os vectores tiverem uma norma diferente de zero, isto quer dizer que formam um 
ângulo de \(90^0\) entre si. Se dois vectores forem ortogonais e tiverem norma unitária, chamam-se <b>ortonormais</b>.
<h3>Matriz ortogonal</h3>
É uma matriz quadrada cujas linhas são mutualmente ortonormais e cujas colunas são mutualmente ortonormais:<p>
	\[A^TA = AA^T = I\]
</p>
Isto implica que:<p>
	\[A^{-1} = A^T\]
</p>
Estas matrizes são interessantes porque a sua inversa é computacionalmente fácil de obter.
<p>
	<a href="../jupyter-notebook/Algebra Linear.ipynb">Exemplo de matrizes com numpy e tensorflow</a>
</p>