---
layout: post
title:  Rede Neuronal - Afinar os Hyperparâmetros
date:   2020-06-25 16:00:00
---

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<h2>Introdução</h2>
A flexibilidade de uma rede neuronal, proporcionada pelos diversos hyperparâmetros e das combinações entre eles que 
podemos fazer, traz-nos um drama: como seleccionar a melhor combinação entre eles. O problema é mesmo este,
são muitos hyperparâmetroe que podemos combinar. Até um caso simples de uma MLP (Multi Layer Perecepton), em termos de
hyperparâmetros, podemos seleccionar o nº de camadas (<i>layers</i>), o número de unidades (<i>neurons</i>) por
camada, a função de activação para cada camada, inicialização dos pesos (<i>weights</i>), a taxa de 
aprendizagem (<i>learning rate</i>), entre outros.
<br>
A opção mais simples é tentar várias combinações e ver qual destas traz melhores resultados no conjunto de dados
de validação.
<br>
Por exemplo, podemos usar o <b><i>sklearn.model_selection.RandomizedSearchCV</i></b> para explorar o espaço dos hyperparâmetros.
<p>
Nota: link para o jupyter notebook com o código completo: <br>
<a href="https://github.com/mleiria/mleiria.github.io/blob/master/jupyter-notebook/FineTunning_NN-Hyperparams.ipynb" target="_blank">MLP_Regression_Synthetic_Data.ipynb</a>
<br>ou Colab:<br>
<a href="https://colab.research.google.com/drive/1ETQe_CNwZGQY-m9NkPhhc8hPlQOdan11?usp=sharing" target="_blank">MLP_Regression_Synthetic_Data.ipynb</a>
</p>

<h2>Modelo Keras</h2>
Comecemos por importar as classes necessárias.
<pre>
import sklearn
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import reciprocal
from tensorflow import keras
from sklearn.model_selection import train_test_split
import numpy as np

print('The scikit-learn version is {}.'.format(sklearn.__version__))
</pre>
<b>out:</b>
<pre>
The scikit-learn version is 0.21.2.
</pre>
Criar uma função que constrói e compila um modelo Keras, dados um conjunto de hyperparâmetros:
<pre>
def build_model(n_hidden=3, n_neurons=30, lr=1e-3, input_shape=[3]):
    """
    Constrói um modelo Keras com base nos parâmetros de entrada.
    
    :param n_hidden: Número de camadas escondidas (hidden layers)
    :param n_neurons: Número de unidades por camada
    :param lr: Taxa de aprendizagem (learning rate)
    :param input_shape: Formato dos dados de entrada
    :return: Um modelo Keras do tipo Sequencial
    """
    model = keras.models.Sequential()
    model.add(keras.layers.InputLayer(input_shape=input_shape))
    # com n_hidden camadas densas escondidas
    # Função de activação relu
    for i in range(n_hidden):
        model.add(keras.layers.Dense(n_neurons, activation='relu'))
    # Camada de saída
    # Perda medida pelo mse (mean squared error)
    # Optimizador SGD (Stochastic Gradiente Descent)
    model.add(keras.layers.Dense(1))
    model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=lr))
    return model
</pre>
A função supra cria um modelo Sequencial simples para uma regressão univariada (só um neurónio de saída),
com um dado formato (ou dimensão) de dados de entrada (no exemplo, 3 <i>features</i>) e dados o número de
camadas e neurónios por camada. De seguida o modelo é compilado e usa o optimizador SGD com a taxa de aprendizagem
também ela especificada como parâmetro de entrada. É boa prática fornecer sempre valores, razoáveis, por defeito 
ao maior número possível de hyperparâmetros.
<br>
<h3>Criar um regressor <b><i>KerasRegressor</i></b> com base na função <b><i>build_model()</i></b></h3>
<pre>keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)</pre>
O objecto <b><i>KerasRegressor</i></b>é um <i>wrapper</i> sobre o modelo Keras construído. Como não 
especificámos nenhum hyperparâmetro na sua criação, ele vai usar os valores por defeito que foram definidos
no <b><i>build_model()</i></b>. Agora podemos usar este objecto como um regressor Scikit-Learn, i.e., podemos
treiná-lo usando o método <b><i>fit()</i></b> e posteriormente avaliá-lo com o método <b><i>score()</i></b>. 
Finalmente, para as previsões usamos o método <b><i>predict()</i></b>. O código seguinte exemplifica:

<pre>
keras_reg.fit(x_train, y_train, epochs=100,
                  validation_data=(x_valid, y_valid),
                  callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])

mse_test = keras_reg.score(x_test, y_test)
y_pred = keras_reg.predict(x_new
</pre>
 Qualquer parâmetro extra que passe para o método <b><i>fit()</i></b>, será passado para o modelo
 Keras subjacente. Atenção que o <i>score</i> é o oposto do <i>MSE</i> porque o Scikit-Learn recebe
 <i>scores</i> e não perdas (i.e., quanto maior, melhor).
 <p>
 Agora o que queremos é testar várias combinações de hyperparâmetros e ver qual delas é a melhor. Como
 temos muitos hyperparâmetros, a pesquisa aleatória (<i>randomized search</i>) faz mais sentido do que a pesquisa
 em rede (<i>grid search</i>). Exploremos o número de camadas escondidas, neurónios e taxa de aprendizagem:
 </p>
<pre>
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import reciprocal

search_params = {
    'n_hidden': [1, 3, 5, 7],
    'n_neurons': np.arange(1, 100),
    'lr': reciprocal(3e-4, 3e-2)
}

rnd_search = RandomizedSearchCV(keras_reg, search_params, cv=3, n_iter=10)
rnd_search.fit(x_train, y_train, epochs=100, validation_data=(x_valid, y_valid),
               callbacks=[keras.callbacks.EarlyStopping(patience=10)])	
</pre>

Estes parâmetros extra que passamos para o método <b><i>fit()</i></b> serão passados para o modelo Keras.
Note-se que o <b><i>RandomizedSearchCV</i></b> usa a validação <i>K-fold cross validation</i> de forma que 
não usa <b><i>x_valid</i></b> e <b><i>y_valid</i></b>. Estes só servem para o <b><i>EarlyStopping</i></b>.
Quando este procedimento acabar, podemos aceder aos melhores parâmetros e ao melhor score:

<pre>
print(rnd_search.best_params_)
print(rnd_search.best_score_)
</pre>

