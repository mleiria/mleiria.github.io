<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <meta name="generator" content="Jekyll">

  <title>Rede Neuronal - Afinar os Hyperparâmetros</title>

  <link rel="stylesheet" href="/css/main.css">
  
  <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" /> <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Rede Neuronal - Afinar os Hyperparâmetros | Deep Learning-Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Rede Neuronal - Afinar os Hyperparâmetros" />
<meta name="author" content="MLeiria" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A Deep Learning Blog" />
<meta property="og:description" content="A Deep Learning Blog" />
<link rel="canonical" href="http://localhost:4000/FT_NN_Hyperparams" />
<meta property="og:url" content="http://localhost:4000/FT_NN_Hyperparams" />
<meta property="og:site_name" content="Deep Learning-Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-25T17:00:00+01:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"http://localhost:4000/FT_NN_Hyperparams","headline":"Rede Neuronal - Afinar os Hyperparâmetros","dateModified":"2020-06-25T17:00:00+01:00","datePublished":"2020-06-25T17:00:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/FT_NN_Hyperparams"},"author":{"@type":"Person","name":"MLeiria"},"description":"A Deep Learning Blog","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>

<body>
  <div id="wrapper">
    <header>
  <div>
    <a href="/">
    
    <h1>deeplearning@home:~$</h1>
    </a>
    <div class="header-links">
      <a href="/archive"><h2 class="header-link">Archive</h2></a>
<a href="/about"><h2 class="header-link">About</h2></a>
<a href="/atom.xml"><h2 class="header-link">RSS</h2></a>
    </div>
  </div>
</header>
    <div class="container">
      <section id="main_content">
        <article>
  <h2>Rede Neuronal - Afinar os Hyperparâmetros</h2>
  <time datetime="2020-06-25T17:00:00+01:00" class="by-line">25 Jun 2020</time>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<h2>Introdução</h2>
A flexibilidade de uma rede neuronal, proporcionada pelos diversos hyperparâmetros e das combinações entre eles que 
podemos fazer, traz-nos um drama: como seleccionar a melhor combinação entre eles. O problema é mesmo este,
são muitos hyperparâmetroe que podemos combinar. Até um caso simples de uma MLP (Multi Layer Perecepton), em termos de
hyperparâmetros, podemos seleccionar o nº de camadas (<i>layers</i>), o número de unidades (<i>neurons</i>) por
camada, a função de activação para cada camada, inicialização dos pesos (<i>weights</i>), a taxa de 
aprendizagem (<i>learning rate</i>), entre outros.
<br>
A opção mais simples é tentar várias combinações e ver qual destas traz melhores resultados no conjunto de dados
de validação.
<br>
Por exemplo, podemos usar o <b><i>sklearn.model_selection.RandomizedSearchCV</i></b> para explorar o espaço dos hyperparâmetros.
<p>
Nota: link para o jupyter notebook com o código completo: <br>
<a href="https://github.com/mleiria/mleiria.github.io/blob/master/jupyter-notebook/FineTunning_NN-Hyperparams.ipynb" target="_blank">MLP_Regression_Synthetic_Data.ipynb</a>
<br>ou Colab:<br>
<a href="https://colab.research.google.com/drive/1ETQe_CNwZGQY-m9NkPhhc8hPlQOdan11?usp=sharing" target="_blank">MLP_Regression_Synthetic_Data.ipynb</a>
</p>

<h2>Modelo Keras</h2>
Comecemos por importar as classes necessárias.
<pre>
import sklearn
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import reciprocal
from tensorflow import keras
from sklearn.model_selection import train_test_split
import numpy as np

print('The scikit-learn version is {}.'.format(sklearn.__version__))
</pre>
<b>out:</b>
<pre>
The scikit-learn version is 0.21.2.
</pre>
Criar uma função que constrói e compila um modelo Keras, dados um conjunto de hyperparâmetros:
<pre>
def build_model(n_hidden=3, n_neurons=30, lr=1e-3, input_shape=[3]):
    """
    Constrói um modelo Keras com base nos parâmetros de entrada.
    
    :param n_hidden: Número de camadas escondidas (hidden layers)
    :param n_neurons: Número de unidades por camada
    :param lr: Taxa de aprendizagem (learning rate)
    :param input_shape: Formato dos dados de entrada
    :return: Um modelo Keras do tipo Sequencial
    """
    model = keras.models.Sequential()
    model.add(keras.layers.InputLayer(input_shape=input_shape))
    # com n_hidden camadas densas escondidas
    # Função de activação relu
    for i in range(n_hidden):
        model.add(keras.layers.Dense(n_neurons, activation='relu'))
    # Camada de saída
    # Perda medida pelo mse (mean squared error)
    # Optimizador SGD (Stochastic Gradiente Descent)
    model.add(keras.layers.Dense(1))
    model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=lr))
    return model
</pre>
A função supra cria um modelo Sequencial simples para uma regressão univariada (só um neurónio de saída),
com um dado formato (ou dimensão) de dados de entrada (no exemplo, 3 <i>features</i>) e dados o número de
camadas e neurónios por camada. De seguida o modelo é compilado e usa o optimizador SGD com a taxa de aprendizagem
também ela especificada como parâmetro de entrada. É boa prática fornecer sempre valores, razoáveis, por defeito 
ao maior número possível de hyperparâmetros.
<br>
<h2>Criar um regressor <b><i>KerasRegressor</i></b> com base na função <b><i>build_model()</i></b></h2>
<pre>keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)</pre>
O objecto <b><i>KerasRegressor</i></b>é um <i>wrapper</i> sobre o modelo Keras construído. Como não 
especificámos nenhum hyperparâmetro na sua criação, ele vai usar os valores por defeito que foram definidos
no <b><i>build_model()</i></b>. Agora podemos usar este objecto como um regressor Scikit-Learn, i.e., podemos
treiná-lo usando o método <b><i>fit()</i></b> e posteriormente avaliá-lo com o método <b><i>score()</i></b>. 
Finalmente, para as previsões usamos o método <b><i>predict()</i></b>. O código seguinte exemplifica:

<pre>
keras_reg.fit(x_train, y_train, epochs=100,
                  validation_data=(x_valid, y_valid),
                  callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])

mse_test = keras_reg.score(x_test, y_test)
y_pred = keras_reg.predict(x_new
</pre>
 Qualquer parâmetro extra que passe para o método <b><i>fit()</i></b>, será passado para o modelo
 Keras subjacente. Atenção que o <i>score</i> é o oposto do <i>MSE</i> porque o Scikit-Learn recebe
 <i>scores</i> e não perdas (i.e., quanto maior, melhor).
 <p>
 Agora o que queremos é testar várias combinações de hyperparâmetros e ver qual delas é a melhor. Como
 temos muitos hyperparâmetros, a pesquisa aleatória (<i>randomized search</i>) faz mais sentido do que a pesquisa
 em rede (<i>grid search</i>). Exploremos o número de camadas escondidas, neurónios e taxa de aprendizagem:
 </p>
<pre>
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import reciprocal

search_params = {
    'n_hidden': [1, 3, 5, 7],
    'n_neurons': np.arange(1, 100),
    'lr': reciprocal(3e-4, 3e-2)
}

rnd_search = RandomizedSearchCV(keras_reg, search_params, cv=3, n_iter=10)
rnd_search.fit(x_train, y_train, epochs=100, validation_data=(x_valid, y_valid),
               callbacks=[keras.callbacks.EarlyStopping(patience=10)])	
</pre>

Estes parâmetros extra que passamos para o método <b><i>fit()</i></b> serão passados para o modelo Keras.
Note-se que o <b><i>RandomizedSearchCV</i></b> usa a validação <i>K-fold cross validation</i> de forma que 
não usa <b><i>x_valid</i></b> e <b><i>y_valid</i></b>. Estes só servem para o <b><i>EarlyStopping</i></b>.
Quando este procedimento acabar, podemos aceder aos melhores parâmetros e ao melhor score:

<pre>
print(rnd_search.best_params_)
print(rnd_search.best_score_)
</pre>
<h2>Algumas considerações finais</h2>
<h3>Número de camadas escondidas</h3>
Para muitos problemas podemos começar com uma única camada escondida e quase de certeza que obtemos bons resultados.
Em princípio, uma MLP com uma só camada escondida consegue modelar qualquer função desde que tenha neurónios suficientes,
o que não quer dizer que seja a solução óptima. Para problemas complexos temos de encontrar o balanço entre o número de 
camadas escondidas e o número de neurónios por camada.
<h3>Número de neurónios por camada escondida</h3>
O número de neurónios nas camadas de entrada e saída são dependentes do tipo de dados de entrada e pelo tipo de 
tarefa que temos em mãos. No exemplo supra, temos três neurónios de entrada (porque temos três features. No exemplo são 
dados sintéticos mas poderiam ser, por exemplo, o número de divisões de um apartamento, o total em metros quadrados do 
apartamento e a zona e o output seria o preço do apartamento) e um neurónio de saída.<br>
Para as camadas escondidas, era comum  escolher o número de neurónios de forma a fazer uma pirâmide com cada vez menos
neuŕonios em cada camada. Por exemplo, uma tipica rede neuronal para o conjunto de dados <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a>
teria a seguinte arquitectura: 3 camadas escondidas, onde a primeira camada tem 300 neurónios, a segunda 200 e a terceira 100.
Esta prática tem sido abandonada porque a experiência mostra que usando o mesmo número de neurónios em todas as
camadas escondidas traz tão bons, ou melhores resultados, que o esquema em pirâmide. Traz ainda a vantagem de ficarmos
só com um hyperparâmetro para afinar em vez de um por camada. De qualquer forma e dependente do conjunto de dados que 
temos em mãos, às vezes ajuda fazer a primeira camada escondida maior do que as outras.
<p>
Tal como o número de camadas, podemos tentar aumentar o número de neurónios gradualmente até a rede começar a 
fazer <i>overfitting</i>.Na prática é muito mais simples e eficiente escolher um modelo com mais camadas e 
mais neurónios do que o necessário e depois usar <i>early stopping</i> e outras técnicas de regularização para prevenir o 
<i>overfitting.</i>
</p>
<h3>Taxa de aprendizagem</h3>
Sem dúvida o hyperparâmetro mais importante. Uma boa forma de encontrar o melhor valor é treinar o modelo durante 
umas centenas de iterações , começando com um valor bastante baixo (e.g. \(10^{-5}\)) e gradualmente ir aumentando 
até um valor alto (e.g., 10). Acompanhar este exercício com uma visualização gráfica da função de perda em vs taxa
de aprendizagem.
<h3>Optimizador</h3>
Há vários para além do clássico <i>Mini-batch Gradient Descent</i> e fazer uma escolha cuidada é importante.
<h3>Tamanho do batch</h3>
Esta escolha é muito dependente do hardware que temos em mãos para treinar a rede. Valores elevados
para o tamanho do batch requeremm aceleradores de hardware (GPUs).
<h3>Função de activação</h3>
De uma forma geral, a função de activação ReLU é uma boa escolha para ser usada pelas camadas escondidas. Para a 
camada de saída (<i>output layer</i>), depende da tarefa em mãos.
</article>
      </section>
    </div>
  </div>

   <footer>
  <a href="https://creativecommons.org/licenses/by-nc/3.0/deed.en_US">
    <span>
        <b>DeepLearning</b>
    </span>
    
    <span>© 2021</span>
  </a>
</footer>

  
</body>

</html>