<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning-Blog</title>
    <description>A Deep Learning Blog</description>
    <link>http://localhost:4000</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <author>
      <name>MLeiria</name>
      <email>manuel.leiria@gmail.com</email>
      <uri>https://ashishchaudhary.in/hacker-blog</uri>
    </author>
    
      <item>
        <title>Métodos Numéricos - Bisecção (implementação em Scala)</title>
        <description>&lt;script src=&quot;https://polyfill.io/v3/polyfill.min.js?features=es6&quot;&gt;&lt;/script&gt;
&lt;script id=&quot;MathJax-script&quot; async
          src=&quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&quot;&gt;
&lt;/script&gt;

&lt;h2&gt;Introdução&lt;/h2&gt;
&lt;p&gt;O método da Bisecção (ver &lt;a href=&quot;https://mleiria.github.io/Metodos_Numericos-Biseccao&quot;&gt;Metodos_Numericos-Biseccao&lt;/a&gt;) mas com uma implementação em Scala&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/mleiria/a7c1b9ddb9422e74cd33d2197fefb281.js&quot;&gt;&lt;/script&gt;
&lt;p&gt;
&lt;pre&gt;
0    1,000000000000    -2,000000000000    
1    1,500000000000    -0,125000000000    
2    1,750000000000    1,609375000000    
3    1,625000000000    0,666015625000    
4    1,562500000000    0,252197265625    
5    1,531250000000    0,059112548828    
6    1,515625000000    -0,034053802490    
7    1,523437500000    0,012250423431    
8    1,519531250000    -0,010971248150    
9    1,521484375000    0,000622175634    
10    1,520507812500    -0,005178886466    
11    1,520996093750    -0,002279443317    
12    1,521240234375    -0,000828905861    
13    1,521362304688    -0,000103433124    
14    1,521423339844    0,000259354252    
15    1,521392822266    0,000077956314    
16    1,521377563477    -0,000012739468    
17    1,521385192871    0,000032608157    
18    1,521381378174    0,000009934278    
19    1,521379470825    -0,000001402611    
20    1,521380424500    0,000004265829    
21    1,521379947662    0,000001431608    
22    1,521379709244    0,000000014498    
23    1,521379590034    -0,000000694057    
24    1,521379649639    -0,000000339779    
25    1,521379679441    -0,000000162641    
26    1,521379694343    -0,000000074071    
27    1,521379701793    -0,000000029787    
28    1,521379705518    -0,000000007644    
29    1,521379707381    0,000000003427    
30    1,521379706450    -0,000000002109    
31    1,521379706915    0,000000000659    
32    1,521379706683    -0,000000000725    
1.5213797067990527
&lt;/pre&gt;


</description>
        <pubDate>Fri, 10 Dec 2021 13:00:00 +0000</pubDate>
        <link>http://localhost:4000//Metodos_Numericos-Biseccao_ScalaImpl</link>
        <link href="http://localhost:4000/Metodos_Numericos-Biseccao_ScalaImpl"/>
        <guid isPermaLink="true">http://localhost:4000/Metodos_Numericos-Biseccao_ScalaImpl</guid>
      </item>
    
      <item>
        <title>Amazon AWS. Cheat Sheet.</title>
        <description>&lt;script src=&quot;https://polyfill.io/v3/polyfill.min.js?features=es6&quot;&gt;&lt;/script&gt;
&lt;script id=&quot;MathJax-script&quot; async
          src=&quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&quot;&gt;
&lt;/script&gt;
&lt;style&gt;
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
&lt;/style&gt;
&lt;small&gt;Notes taken form &lt;a href=&quot;https://www.oreilly.com/library/view/machine-learning-with/9781789806199/&quot;&gt;Machine Learning with AWS&lt;/a&gt;&lt;/small&gt;

&lt;h2&gt;Amazon S3&lt;/h2&gt;
&lt;h3&gt;Intro&lt;/h3&gt;
	&lt;ul&gt;
		&lt;li&gt;&lt;strong&gt;S3&lt;/strong&gt; is an online cloud object sotrage and retrieval service.&lt;/li&gt;
		&lt;li&gt;Is a place to store and retrieve files (text files, images, audio, videio,...)&lt;/li&gt;
		&lt;li&gt;Can easly be used in conjuntion with additional AWS Machine Learning and infrastructure services.&lt;/li&gt;
		&lt;li&gt;S3 is used to store almost any type of file. We can view it as a sort of file system and the folders, are called &lt;strong&gt;Buckets&lt;/strong&gt;.&lt;/li&gt;
		&lt;li&gt;A file in a traditional filesystem is an &lt;strong&gt;object&lt;/strong&gt; in S3. You cannot have buckets inside buckets.&lt;br&gt;&lt;/li&gt;
		&lt;li&gt;Objects stored in Buckets can be accessed from a web server endpoint.&lt;/li&gt;
	&lt;/ul&gt;

&lt;h3&gt;Core S3 Concepts&lt;/h3&gt;
&lt;strong&gt;Type of data storable&lt;/strong&gt;: text files, imagesm audio, video,... &lt;br&gt;
&lt;strong&gt;Objects&lt;/strong&gt;: are the most basic entity stored in S3. Every object contains data, metadata and a key.&lt;br&gt;
&lt;strong&gt;Keys&lt;/strong&gt;: is the name assigned to an object that uniquely identifies an object inside a bucket.&lt;br&gt;
&lt;strong&gt;Bucket&lt;/strong&gt;: is the container where you store objects. Are created at root level.You can have multiple buckets , but you cannot have buckets inside buckets.&lt;br&gt;
&lt;strong&gt;Region&lt;/strong&gt;: refers to the geographical region wheer Amazon S3 stores a Bucket. the pbject storage in a Bucket with different forms is as follows:&lt;br&gt;&lt;br&gt;

 https://s3.amazonaws.com/myBucket/pos_sentiment_leaves_of_grass.txt
&lt;br&gt;&lt;br&gt; 
where:&lt;br&gt;
&lt;ul&gt;
&lt;li&gt; https://s3.amazonaws.com -&gt; S3 Endpoint&lt;/li&gt;	
&lt;li&gt;myBucket -&gt; Bucket Name&lt;/li&gt;
&lt;li&gt;pos_sentiment_leaves_of_grass.txt -&gt; Key&lt;/li&gt;
&lt;li&gt;myBucket/pos_sentiment_leaves_of_grass.txt -&gt; Globally unique key&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;S3 Operations&lt;/h3&gt;
The S3 API includes the following operations:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Bucket&lt;/strong&gt;: Create, Delete and List keys in a Bucket.&lt;/li&gt;	
&lt;li&gt;&lt;strong&gt;Object&lt;/strong&gt;: Write, Read and Delete.&lt;/li&gt;	
&lt;/ul&gt;
&lt;/p&gt;
&lt;h2&gt;Summarizing Text Documents Using NLP&lt;/h2&gt;
&lt;h3&gt;Amazon Comprehend&lt;/h3&gt;
&lt;ul&gt;
	&lt;li&gt;Is a Natural Language Processing (NLP) Service.&lt;/li&gt;
	&lt;li&gt;Processes any text file in UTF-8 format. It uses a pre-trained model to examine a document or set of documents, in order to gather insights about the document set.&lt;/li&gt;
	&lt;li&gt;Amazon continuosly trains the model so there'se no need to provide training dadta.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;First Step: Analyze the text data to determine the dominant language.&lt;/h4&gt;
For this step we have to operations available:
&lt;ol&gt;
	&lt;li&gt;&lt;strong&gt;DetectDominantLanguage&lt;/strong&gt;:accepts a UTF-8 text string greater than 20 characters and less than 5,000 bytes of UTF-8 encoded characters.&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;BatchDetectDominantLanguage&lt;/strong&gt;:accepts an array of strings as a list wiht a maximum of 25 documents. For each document the above rule for the size applies.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;DetectDominantLanguage&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;
# import the AWS SDK for python (boto3) - http://boto3.readthedocs.io/en/latest/
import boto3
# import json module to serialize JSON - https://docs.python.org/3.6/library/json.html
import json

# instantiate a new comprehend client
comprehend = boto3.client(service_name='comprehend', region_name='eu-west-2')

# provide english and spanish text to analyze
english_string = 'Machine Learning is fascinating.'
spanish_string = 'El aprendizaje automático es fascinante.'
portuguese_string = 'Olá, eu chamo-me Manuel Aleixo de Sousa Leiria'

print('Calling DetectDominantLanguage')

print('english_string result:')
# json.dumps() writes JSON data to a Python string
print(json.dumps(comprehend.detect_dominant_language(Text = english_string), sort_keys=True, indent=4))

print('\n spanish_string result:')
print(json.dumps(comprehend.detect_dominant_language(Text = spanish_string), sort_keys=True, indent=4))
print('End of DetectDominantLanguage\n')

print('\n portuguese_string result:')
print(json.dumps(comprehend.detect_dominant_language(Text = portuguese_string), sort_keys=True, indent=4))
print('End of DetectDominantLanguage\n')	

&lt;/pre&gt;
Output:
&lt;pre&gt;
Calling DetectDominantLanguage
english_string result:
{
    &quot;Languages&quot;: [
        {
            &quot;LanguageCode&quot;: &quot;en&quot;,
            &quot;Score&quot;: 0.9943646192550659
        }
    ],
    &quot;ResponseMetadata&quot;: {
        &quot;HTTPHeaders&quot;: {
            &quot;content-length&quot;: &quot;64&quot;,
            &quot;content-type&quot;: &quot;application/x-amz-json-1.1&quot;,
            &quot;date&quot;: &quot;Mon, 07 Jun 2021 17:31:03 GMT&quot;,
            &quot;x-amzn-requestid&quot;: &quot;0dcc2001-6c79-4dae-8fe8-c76ef316814e&quot;
        },
        &quot;HTTPStatusCode&quot;: 200,
        &quot;RequestId&quot;: &quot;0dcc2001-6c79-4dae-8fe8-c76ef316814e&quot;,
        &quot;RetryAttempts&quot;: 0
    }
}

 spanish_string result:
{
    &quot;Languages&quot;: [
        {
            &quot;LanguageCode&quot;: &quot;es&quot;,
            &quot;Score&quot;: 0.9700182676315308
        }
    ],
    &quot;ResponseMetadata&quot;: {
        &quot;HTTPHeaders&quot;: {
            &quot;content-length&quot;: &quot;64&quot;,
            &quot;content-type&quot;: &quot;application/x-amz-json-1.1&quot;,
            &quot;date&quot;: &quot;Mon, 07 Jun 2021 17:31:03 GMT&quot;,
            &quot;x-amzn-requestid&quot;: &quot;2b463941-89eb-42a3-b577-45ed0fd351ee&quot;
        },
        &quot;HTTPStatusCode&quot;: 200,
        &quot;RequestId&quot;: &quot;2b463941-89eb-42a3-b577-45ed0fd351ee&quot;,
        &quot;RetryAttempts&quot;: 0
    }
}
End of DetectDominantLanguage


 portuguese_string result:
{
    &quot;Languages&quot;: [
        {
            &quot;LanguageCode&quot;: &quot;pt&quot;,
            &quot;Score&quot;: 0.991827130317688
        }
    ],
    &quot;ResponseMetadata&quot;: {
        &quot;HTTPHeaders&quot;: {
            &quot;content-length&quot;: &quot;63&quot;,
            &quot;content-type&quot;: &quot;application/x-amz-json-1.1&quot;,
            &quot;date&quot;: &quot;Mon, 07 Jun 2021 17:31:03 GMT&quot;,
            &quot;x-amzn-requestid&quot;: &quot;4d83167b-9e50-4d3a-9e9f-a8c5c521c159&quot;
        },
        &quot;HTTPStatusCode&quot;: 200,
        &quot;RequestId&quot;: &quot;4d83167b-9e50-4d3a-9e9f-a8c5c521c159&quot;,
        &quot;RetryAttempts&quot;: 0
    }
}
End of DetectDominantLanguage
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;DetectDominantLanguage&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;
# import the AWS SDK for python (boto3) - http://boto3.readthedocs.io/en/latest/
import boto3
# import json module to serialize JSON - https://docs.python.org/3.6/library/json.html
import json

# instantiate a new comprehend client
comprehend = boto3.client(service_name='comprehend', region_name='eu-west-2')

# provide english and spanish text to analyze
english_string_list = ['Machine Learning is fascinating.', 'Studying Artificial Intelligence is my passion.'] 
spanish_string_list = ['El aprendizaje automático es fascinante.', 'Estudiar Inteligencia Artificial es mi pasión.']

print('Calling BatchDetectDominantLanguage')

print('english_string_list results:')
# json.dumps() writes JSON data to a Python string
print(json.dumps(comprehend.batch_detect_dominant_language(TextList = english_string_list), sort_keys=True, indent=4))

print('\nspanish_string_list results:')
print(json.dumps(comprehend.batch_detect_dominant_language(TextList = spanish_string_list), sort_keys=True, indent=4))
print('End of BatchDetectDominantLanguage\n')
&lt;/pre&gt;
Output:
&lt;pre&gt;
Calling BatchDetectDominantLanguage
english_string_list results:
{
    &quot;ErrorList&quot;: [],
    &quot;ResponseMetadata&quot;: {
        &quot;HTTPHeaders&quot;: {
            &quot;content-length&quot;: &quot;180&quot;,
            &quot;content-type&quot;: &quot;application/x-amz-json-1.1&quot;,
            &quot;date&quot;: &quot;Mon, 07 Jun 2021 17:40:46 GMT&quot;,
            &quot;x-amzn-requestid&quot;: &quot;809271ff-8f5f-41a3-9461-cdf9746fad45&quot;
        },
        &quot;HTTPStatusCode&quot;: 200,
        &quot;RequestId&quot;: &quot;809271ff-8f5f-41a3-9461-cdf9746fad45&quot;,
        &quot;RetryAttempts&quot;: 0
    },
    &quot;ResultList&quot;: [
        {
            &quot;Index&quot;: 0,
            &quot;Languages&quot;: [
                {
                    &quot;LanguageCode&quot;: &quot;en&quot;,
                    &quot;Score&quot;: 0.9943646192550659
                }
            ]
        },
        {
            &quot;Index&quot;: 1,
            &quot;Languages&quot;: [
                {
                    &quot;LanguageCode&quot;: &quot;en&quot;,
                    &quot;Score&quot;: 0.989561140537262
                }
            ]
        }
    ]
}

spanish_string_list results:
{
    &quot;ErrorList&quot;: [],
    &quot;ResponseMetadata&quot;: {
        &quot;HTTPHeaders&quot;: {
            &quot;content-length&quot;: &quot;181&quot;,
            &quot;content-type&quot;: &quot;application/x-amz-json-1.1&quot;,
            &quot;date&quot;: &quot;Mon, 07 Jun 2021 17:40:46 GMT&quot;,
            &quot;x-amzn-requestid&quot;: &quot;6826ea2c-6c23-4f7f-bf3b-50f2ef488bc5&quot;
        },
        &quot;HTTPStatusCode&quot;: 200,
        &quot;RequestId&quot;: &quot;6826ea2c-6c23-4f7f-bf3b-50f2ef488bc5&quot;,
        &quot;RetryAttempts&quot;: 0
    },
    &quot;ResultList&quot;: [
        {
            &quot;Index&quot;: 0,
            &quot;Languages&quot;: [
                {
                    &quot;LanguageCode&quot;: &quot;es&quot;,
                    &quot;Score&quot;: 0.9700182676315308
                }
            ]
        },
        {
            &quot;Index&quot;: 1,
            &quot;Languages&quot;: [
                {
                    &quot;LanguageCode&quot;: &quot;es&quot;,
                    &quot;Score&quot;: 0.9737135171890259
                }
            ]
        }
    ]
}
End of BatchDetectDominantLanguage
&lt;/pre&gt;
&lt;h4&gt;Detect Named Entities&lt;/h4&gt;
A entity is a textual reference to a unique name of a real-world object, cush as people, places, commercial items, dates, quantities,...&lt;br&gt;
Like the language detection, Entities also have a score to indicate the confidence level that the entity type was detected correctly.&lt;br&gt;
&lt;br&gt;
AWS Comprehend entity types and descriptions:
&lt;p&gt;
&lt;img src=&quot;../images/2021-06-04-Amazon_AWS_1.png&quot; class=&quot;center&quot;/&gt;
&lt;/p&gt;
&lt;h4&gt;Detect Entities - Example&lt;/h4&gt;
&lt;pre&gt;
# import the AWS SDK for python (boto3) - http://boto3.readthedocs.io/en/latest/
import boto3
# import json module to serialize JSON - https://docs.python.org/3.6/library/json.html
import json

# instantiate a new comprehend client
comprehend = boto3.client(service_name='comprehend', region_name='eu-west-2')

# provide english text to analyze
english_string = &quot;I study Machine Learning in Lisboa on Sunday.&quot;

print('Calling DetectEntities')
# json.dumps() writes JSON data to a Python string
print(json.dumps(comprehend.detect_entities(Text = english_string, LanguageCode='en'), 
		sort_keys=True, indent=4))
print('End of DetectEntities\n')    	
&lt;/pre&gt;
Output:
&lt;pre&gt;
	Calling DetectEntities
{
    &quot;Entities&quot;: [
        {
            &quot;BeginOffset&quot;: 16,
            &quot;EndOffset&quot;: 24,
            &quot;Score&quot;: 0.6715667843818665,
            &quot;Text&quot;: &quot;Learning&quot;,
            &quot;Type&quot;: &quot;OTHER&quot;
        },
        {
            &quot;BeginOffset&quot;: 28,
            &quot;EndOffset&quot;: 34,
            &quot;Score&quot;: 0.9909079074859619,
            &quot;Text&quot;: &quot;Lisboa&quot;,
            &quot;Type&quot;: &quot;LOCATION&quot;
        },
        {
            &quot;BeginOffset&quot;: 38,
            &quot;EndOffset&quot;: 44,
            &quot;Score&quot;: 0.9938769936561584,
            &quot;Text&quot;: &quot;Sunday&quot;,
            &quot;Type&quot;: &quot;DATE&quot;
        }
    ],
    &quot;ResponseMetadata&quot;: {
        &quot;HTTPHeaders&quot;: {
            &quot;content-length&quot;: &quot;294&quot;,
            &quot;content-type&quot;: &quot;application/x-amz-json-1.1&quot;,
            &quot;date&quot;: &quot;Mon, 14 Jun 2021 17:35:06 GMT&quot;,
            &quot;x-amzn-requestid&quot;: &quot;5258b705-02c7-4b3e-a1cc-a95d958f1654&quot;
        },
        &quot;HTTPStatusCode&quot;: 200,
        &quot;RequestId&quot;: &quot;5258b705-02c7-4b3e-a1cc-a95d958f1654&quot;,
        &quot;RetryAttempts&quot;: 0
    }
}
End of DetectEntities
&lt;/pre&gt;
&lt;strong&gt;Lisboa&lt;/strong&gt; was detected as a &lt;strong&gt;LOCATION&lt;/strong&gt; and &lt;strong&gt;Sunday&lt;/strong&gt; was detected as the &lt;strong&gt;DATE&lt;/strong&gt;, both with confidence levels ~0.99.&lt;p&gt;
&lt;h4&gt;Detecting Key Phrases&lt;/h4&gt;
A key phrase for AWS is analogous to a noun phrase, which represent an actual thing.
&lt;h4&gt;Detecting Key Phrases - Example&lt;/h4&gt;
&lt;pre&gt;
# import the AWS SDK for python (boto3) - http://boto3.readthedocs.io/en/latest/
import boto3
# import json module to serialize JSON - https://docs.python.org/3.6/library/json.html
import json

# instantiate a new comprehend client
comprehend = boto3.client(service_name='comprehend', region_name='eu-west-2') #us-east-1

# provide english text to analyze
english_string = 'I study Machine Learning in Lisbon on Thursday and it was great'

print('Calling DetectKeyPhrases')
# json.dumps() writes JSON data to a Python string
print(json.dumps(comprehend.detect_key_phrases(Text = english_string, LanguageCode='en'),
				 sort_keys=True, indent=4))
print('End of DetectKeyPhrases\n')
&lt;/pre&gt;
Output:
&lt;pre&gt;
Calling DetectKeyPhrases
{
    &quot;KeyPhrases&quot;: [
        {
            &quot;BeginOffset&quot;: 8,
            &quot;EndOffset&quot;: 24,
            &quot;Score&quot;: 1.0,
            &quot;Text&quot;: &quot;Machine Learning&quot;
        },
        {
            &quot;BeginOffset&quot;: 28,
            &quot;EndOffset&quot;: 34,
            &quot;Score&quot;: 1.0,
            &quot;Text&quot;: &quot;Lisbon&quot;
        },
        {
            &quot;BeginOffset&quot;: 38,
            &quot;EndOffset&quot;: 46,
            &quot;Score&quot;: 1.0,
            &quot;Text&quot;: &quot;Thursday&quot;
        }
    ],
    &quot;ResponseMetadata&quot;: {
        &quot;HTTPHeaders&quot;: {
            &quot;content-length&quot;: &quot;213&quot;,
            &quot;content-type&quot;: &quot;application/x-amz-json-1.1&quot;,
            &quot;date&quot;: &quot;Tue, 15 Jun 2021 08:00:45 GMT&quot;,
            &quot;x-amzn-requestid&quot;: &quot;35543e2c-14d0-463e-a4ea-81bdb705e6f0&quot;
        },
        &quot;HTTPStatusCode&quot;: 200,
        &quot;RequestId&quot;: &quot;35543e2c-14d0-463e-a4ea-81bdb705e6f0&quot;,
        &quot;RetryAttempts&quot;: 0
    }
}
End of DetectKeyPhrases
&lt;/pre&gt;
&lt;h4&gt;Detecting Sentiments&lt;/h4&gt;

to be continued...



</description>
        <pubDate>Fri, 04 Jun 2021 13:00:00 +0100</pubDate>
        <link>http://localhost:4000//Amazon_AWS</link>
        <link href="http://localhost:4000/Amazon_AWS"/>
        <guid isPermaLink="true">http://localhost:4000/Amazon_AWS</guid>
      </item>
    
      <item>
        <title>Métodos Numéricos - Bisecção</title>
        <description>&lt;script src=&quot;https://polyfill.io/v3/polyfill.min.js?features=es6&quot;&gt;&lt;/script&gt;
&lt;script id=&quot;MathJax-script&quot; async
          src=&quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&quot;&gt;
&lt;/script&gt;

&lt;h2&gt;Introdução&lt;/h2&gt;
&lt;p&gt;O método da Bisecção é um método numérico que permite calcular o valor (aproximado) das raizes de uma equação não linear.&lt;/p&gt;
Para garantir a solução, usando este método, as seguintes condições têm de se verificar:
&lt;p&gt;
	&lt;ol&gt;
		&lt;li&gt;&lt;i&gt;f&lt;/i&gt; contínua em &lt;i&gt;[a, b]&lt;/i&gt;&lt;/li&gt;
		&lt;li&gt;&lt;i&gt;f(a)f(b) &lt; 0&lt;/i&gt;&lt;/li&gt;
		&lt;li&gt;Só existe um zero no interior do intervalo &lt;i&gt;[a, b]&lt;/i&gt;&lt;/li&gt;
	
&lt;/ol&gt;
&lt;/p&gt;
&lt;p&gt;
	As condições (1) e (2) sugerem-nos um processo bastante simples para obter uma aproximação do zero de uma função. Supondo que só existe um zero da função &lt;i&gt;f&lt;/i&gt; no interior do intervalo
	&lt;i&gt;[a, b]&lt;/i&gt; (condição (3)), o processo consiste em dividir o intervalo dado ao meio e testar de novo a condição (2) nos subintervalos \[ [a, \frac{a + b}{2}]\] e \[ [ \frac{a + b}{2}, b] \]
	para determinar qual deles contém a raiz. O processo é repetido para o novo subintervalo até que se obtenha uma precisão prefixada.

&lt;/p&gt;
&lt;h2&gt;Pseudocódigo&lt;/h2&gt;
&lt;strong&gt;Condição suficiente de convergência do método&lt;/strong&gt;&lt;p&gt;
&lt;i&gt;- f contínua em [a, b]&lt;/i&gt;&lt;br&gt;
&lt;i&gt;- f(a)f(b) &lt; 0&lt;/i&gt;&lt;p&gt;
&lt;strong&gt;Inicialização&lt;/strong&gt;&lt;p&gt;
&lt;i&gt;a&lt;sub&gt;0&lt;/sub&gt; = a, b&lt;sub&gt;0&lt;/sub&gt; = b, x&lt;sub&gt;0&lt;/sub&gt; = a ou x&lt;sub&gt;0&lt;/sub&gt; = b&lt;/i&gt;&lt;p&gt;
&lt;strong&gt;Ciclo&lt;/strong&gt;&lt;p&gt;
Para \[ m \ge 0 \] fazer \[ x_{m+1} = \frac{a_m + b_m}{2}\]
Se \( |x_{m+1} - x_m | \le \epsilon \) ou \( |f(x_{m+1})| \le \epsilon \) &lt;p&gt;
então fazer &lt;i&gt;x&lt;sub&gt;m+1&lt;/sub&gt;&lt;/i&gt; e retornar. &lt;p&gt;
Caso contrário: &lt;p&gt;
Se \[ f(x_{m+1})f(a_m) &lt; 0\] então fazer:&lt;p&gt; \( a_{m+1} = a_m \) e \( b_{m+1} = x_{m+1} \)&lt;p&gt;
Se não fazer: &lt;p&gt;
\(a_{m+1} = x_{m+1} \) e \(b_{m+1} = b_m \)&lt;p&gt;

&lt;h2&gt;Implementação em Java&lt;/h2&gt;
Consideremos a seguinte função:
\[ f(x) = x^3 - x - 2.0\]

e queremos encontrar o zero da funçaõ dentro do intervalo &lt;i&gt;[1.0, 2.0]&lt;/i&gt;
&lt;img src=&quot;../images/2021-05-03-Metodos_Numericos-Biseccao_1.png&quot;&gt;&lt;p&gt;

&lt;h3&gt;Estilo imperativo&lt;/h3&gt;
&lt;script src=&quot;https://gist.github.com/mleiria/0c0d0c03b861d4121505c7dd3f9cb810.js&quot;&gt;&lt;/script&gt;
&lt;p&gt;
&lt;pre&gt;
0    1.000000000000    -2.000000000000    
1    1.500000000000    -0.125000000000    
2    1.750000000000    1.609375000000    
3    1.625000000000    0.666015625000    
4    1.562500000000    0.252197265625    
5    1.531250000000    0.059112548828    
6    1.515625000000    -0.034053802490    
7    1.523437500000    0.012250423431    
8    1.519531250000    -0.010971248150    
9    1.521484375000    0.000622175634    
10    1.520507812500    -0.005178886466    
11    1.520996093750    -0.002279443317    
12    1.521240234375    -0.000828905861    
13    1.521362304688    -0.000103433124    
14    1.521423339844    0.000259354252    
15    1.521392822266    0.000077956314    
16    1.521377563477    -0.000012739468    
17    1.521385192871    0.000032608157    
18    1.521381378174    0.000009934278    
19    1.521379470825    -0.000001402611    
20    1.521380424500    0.000004265829    
21    1.521379947662    0.000001431608    
22    1.521379709244    0.000000014498    
23    1.521379590034    -0.000000694057    
24    1.521379649639    -0.000000339779    
25    1.521379679441    -0.000000162641    
26    1.521379694343    -0.000000074071    
27    1.521379701793    -0.000000029787    
28    1.521379705518    -0.000000007644    
29    1.521379707381    0.000000003427    
30    1.521379706450    -0.000000002109    
31    1.521379706915    0.000000000659    
Optional[1.5213797067990527]



&lt;/pre&gt;
&lt;/p&gt;
&lt;h3&gt;Estilo funcional&lt;/h3&gt;
&lt;script src=&quot;https://gist.github.com/mleiria/d87462383bdde0273bbbe6cefd37b9d8.js&quot;&gt;&lt;/script&gt;
&lt;p&gt;
&lt;pre&gt;
0    1.000000000000    -2.000000000000    
1    1.500000000000    -0.125000000000    
2    1.750000000000    1.609375000000    
3    1.625000000000    0.666015625000    
4    1.562500000000    0.252197265625    
5    1.531250000000    0.059112548828    
6    1.515625000000    -0.034053802490    
7    1.523437500000    0.012250423431    
8    1.519531250000    -0.010971248150    
9    1.521484375000    0.000622175634    
10    1.520507812500    -0.005178886466    
11    1.520996093750    -0.002279443317    
12    1.521240234375    -0.000828905861    
13    1.521362304688    -0.000103433124    
14    1.521423339844    0.000259354252    
15    1.521392822266    0.000077956314    
16    1.521377563477    -0.000012739468    
17    1.521385192871    0.000032608157    
18    1.521381378174    0.000009934278    
19    1.521379470825    -0.000001402611    
20    1.521380424500    0.000004265829    
21    1.521379947662    0.000001431608    
22    1.521379709244    0.000000014498    
23    1.521379590034    -0.000000694057    
24    1.521379649639    -0.000000339779    
25    1.521379679441    -0.000000162641    
26    1.521379694343    -0.000000074071    
27    1.521379701793    -0.000000029787    
28    1.521379705518    -0.000000007644    
29    1.521379707381    0.000000003427    
30    1.521379706450    -0.000000002109    
31    1.521379706915    0.000000000659    
32    1.521379706683    -0.000000000725    
Optional[1.5213797067990527]
&lt;/pre&gt;

</description>
        <pubDate>Mon, 03 May 2021 14:00:00 +0100</pubDate>
        <link>http://localhost:4000//Metodos_Numericos-Biseccao</link>
        <link href="http://localhost:4000/Metodos_Numericos-Biseccao"/>
        <guid isPermaLink="true">http://localhost:4000/Metodos_Numericos-Biseccao</guid>
      </item>
    
      <item>
        <title>Rede Neuronal - Afinar os Hyperparâmetros</title>
        <description>&lt;script src=&quot;https://polyfill.io/v3/polyfill.min.js?features=es6&quot;&gt;&lt;/script&gt;
&lt;script id=&quot;MathJax-script&quot; async
          src=&quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&quot;&gt;
&lt;/script&gt;


&lt;h2&gt;Introdução&lt;/h2&gt;
A flexibilidade de uma rede neuronal, proporcionada pelos diversos hyperparâmetros e das combinações entre eles que 
podemos fazer, traz-nos um drama: como seleccionar a melhor combinação entre eles. O problema é mesmo este,
são muitos hyperparâmetroe que podemos combinar. Até um caso simples de uma MLP (Multi Layer Perecepton), em termos de
hyperparâmetros, podemos seleccionar o nº de camadas (&lt;i&gt;layers&lt;/i&gt;), o número de unidades (&lt;i&gt;neurons&lt;/i&gt;) por
camada, a função de activação para cada camada, inicialização dos pesos (&lt;i&gt;weights&lt;/i&gt;), a taxa de 
aprendizagem (&lt;i&gt;learning rate&lt;/i&gt;), entre outros.
&lt;br&gt;
A opção mais simples é tentar várias combinações e ver qual destas traz melhores resultados no conjunto de dados
de validação.
&lt;br&gt;
Por exemplo, podemos usar o &lt;b&gt;&lt;i&gt;sklearn.model_selection.RandomizedSearchCV&lt;/i&gt;&lt;/b&gt; para explorar o espaço dos hyperparâmetros.
&lt;p&gt;
Nota: link para o jupyter notebook com o código completo: &lt;br&gt;
&lt;a href=&quot;https://github.com/mleiria/mleiria.github.io/blob/master/jupyter-notebook/FineTunning_NN-Hyperparams.ipynb&quot; target=&quot;_blank&quot;&gt;MLP_Regression_Synthetic_Data.ipynb&lt;/a&gt;
&lt;br&gt;ou Colab:&lt;br&gt;
&lt;a href=&quot;https://colab.research.google.com/drive/1ETQe_CNwZGQY-m9NkPhhc8hPlQOdan11?usp=sharing&quot; target=&quot;_blank&quot;&gt;MLP_Regression_Synthetic_Data.ipynb&lt;/a&gt;
&lt;/p&gt;

&lt;h2&gt;Modelo Keras&lt;/h2&gt;
Comecemos por importar as classes necessárias.
&lt;pre&gt;
import sklearn
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import reciprocal
from tensorflow import keras
from sklearn.model_selection import train_test_split
import numpy as np

print('The scikit-learn version is {}.'.format(sklearn.__version__))
&lt;/pre&gt;
&lt;b&gt;out:&lt;/b&gt;
&lt;pre&gt;
The scikit-learn version is 0.21.2.
&lt;/pre&gt;
Criar uma função que constrói e compila um modelo Keras, dados um conjunto de hyperparâmetros:
&lt;pre&gt;
def build_model(n_hidden=3, n_neurons=30, lr=1e-3, input_shape=[3]):
    &quot;&quot;&quot;
    Constrói um modelo Keras com base nos parâmetros de entrada.
    
    :param n_hidden: Número de camadas escondidas (hidden layers)
    :param n_neurons: Número de unidades por camada
    :param lr: Taxa de aprendizagem (learning rate)
    :param input_shape: Formato dos dados de entrada
    :return: Um modelo Keras do tipo Sequencial
    &quot;&quot;&quot;
    model = keras.models.Sequential()
    model.add(keras.layers.InputLayer(input_shape=input_shape))
    # com n_hidden camadas densas escondidas
    # Função de activação relu
    for i in range(n_hidden):
        model.add(keras.layers.Dense(n_neurons, activation='relu'))
    # Camada de saída
    # Perda medida pelo mse (mean squared error)
    # Optimizador SGD (Stochastic Gradiente Descent)
    model.add(keras.layers.Dense(1))
    model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=lr))
    return model
&lt;/pre&gt;
A função supra cria um modelo Sequencial simples para uma regressão univariada (só um neurónio de saída),
com um dado formato (ou dimensão) de dados de entrada (no exemplo, 3 &lt;i&gt;features&lt;/i&gt;) e dados o número de
camadas e neurónios por camada. De seguida o modelo é compilado e usa o optimizador SGD com a taxa de aprendizagem
também ela especificada como parâmetro de entrada. É boa prática fornecer sempre valores, razoáveis, por defeito 
ao maior número possível de hyperparâmetros.
&lt;br&gt;
&lt;h2&gt;Criar um regressor &lt;b&gt;&lt;i&gt;KerasRegressor&lt;/i&gt;&lt;/b&gt; com base na função &lt;b&gt;&lt;i&gt;build_model()&lt;/i&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;pre&gt;keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)&lt;/pre&gt;
O objecto &lt;b&gt;&lt;i&gt;KerasRegressor&lt;/i&gt;&lt;/b&gt;é um &lt;i&gt;wrapper&lt;/i&gt; sobre o modelo Keras construído. Como não 
especificámos nenhum hyperparâmetro na sua criação, ele vai usar os valores por defeito que foram definidos
no &lt;b&gt;&lt;i&gt;build_model()&lt;/i&gt;&lt;/b&gt;. Agora podemos usar este objecto como um regressor Scikit-Learn, i.e., podemos
treiná-lo usando o método &lt;b&gt;&lt;i&gt;fit()&lt;/i&gt;&lt;/b&gt; e posteriormente avaliá-lo com o método &lt;b&gt;&lt;i&gt;score()&lt;/i&gt;&lt;/b&gt;. 
Finalmente, para as previsões usamos o método &lt;b&gt;&lt;i&gt;predict()&lt;/i&gt;&lt;/b&gt;. O código seguinte exemplifica:

&lt;pre&gt;
keras_reg.fit(x_train, y_train, epochs=100,
                  validation_data=(x_valid, y_valid),
                  callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])

mse_test = keras_reg.score(x_test, y_test)
y_pred = keras_reg.predict(x_new
&lt;/pre&gt;
 Qualquer parâmetro extra que passe para o método &lt;b&gt;&lt;i&gt;fit()&lt;/i&gt;&lt;/b&gt;, será passado para o modelo
 Keras subjacente. Atenção que o &lt;i&gt;score&lt;/i&gt; é o oposto do &lt;i&gt;MSE&lt;/i&gt; porque o Scikit-Learn recebe
 &lt;i&gt;scores&lt;/i&gt; e não perdas (i.e., quanto maior, melhor).
 &lt;p&gt;
 Agora o que queremos é testar várias combinações de hyperparâmetros e ver qual delas é a melhor. Como
 temos muitos hyperparâmetros, a pesquisa aleatória (&lt;i&gt;randomized search&lt;/i&gt;) faz mais sentido do que a pesquisa
 em rede (&lt;i&gt;grid search&lt;/i&gt;). Exploremos o número de camadas escondidas, neurónios e taxa de aprendizagem:
 &lt;/p&gt;
&lt;pre&gt;
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import reciprocal

search_params = {
    'n_hidden': [1, 3, 5, 7],
    'n_neurons': np.arange(1, 100),
    'lr': reciprocal(3e-4, 3e-2)
}

rnd_search = RandomizedSearchCV(keras_reg, search_params, cv=3, n_iter=10)
rnd_search.fit(x_train, y_train, epochs=100, validation_data=(x_valid, y_valid),
               callbacks=[keras.callbacks.EarlyStopping(patience=10)])	
&lt;/pre&gt;

Estes parâmetros extra que passamos para o método &lt;b&gt;&lt;i&gt;fit()&lt;/i&gt;&lt;/b&gt; serão passados para o modelo Keras.
Note-se que o &lt;b&gt;&lt;i&gt;RandomizedSearchCV&lt;/i&gt;&lt;/b&gt; usa a validação &lt;i&gt;K-fold cross validation&lt;/i&gt; de forma que 
não usa &lt;b&gt;&lt;i&gt;x_valid&lt;/i&gt;&lt;/b&gt; e &lt;b&gt;&lt;i&gt;y_valid&lt;/i&gt;&lt;/b&gt;. Estes só servem para o &lt;b&gt;&lt;i&gt;EarlyStopping&lt;/i&gt;&lt;/b&gt;.
Quando este procedimento acabar, podemos aceder aos melhores parâmetros e ao melhor score:

&lt;pre&gt;
print(rnd_search.best_params_)
print(rnd_search.best_score_)
&lt;/pre&gt;
&lt;h2&gt;Algumas considerações finais&lt;/h2&gt;
&lt;h3&gt;Número de camadas escondidas&lt;/h3&gt;
Para muitos problemas podemos começar com uma única camada escondida e quase de certeza que obtemos bons resultados.
Em princípio, uma MLP com uma só camada escondida consegue modelar qualquer função desde que tenha neurónios suficientes,
o que não quer dizer que seja a solução óptima. Para problemas complexos temos de encontrar o balanço entre o número de 
camadas escondidas e o número de neurónios por camada.
&lt;h3&gt;Número de neurónios por camada escondida&lt;/h3&gt;
O número de neurónios nas camadas de entrada e saída são dependentes do tipo de dados de entrada e pelo tipo de 
tarefa que temos em mãos. No exemplo supra, temos três neurónios de entrada (porque temos três features. No exemplo são 
dados sintéticos mas poderiam ser, por exemplo, o número de divisões de um apartamento, o total em metros quadrados do 
apartamento e a zona e o output seria o preço do apartamento) e um neurónio de saída.&lt;br&gt;
Para as camadas escondidas, era comum  escolher o número de neurónios de forma a fazer uma pirâmide com cada vez menos
neuŕonios em cada camada. Por exemplo, uma tipica rede neuronal para o conjunto de dados &lt;a href=&quot;https://en.wikipedia.org/wiki/MNIST_database&quot;&gt;MNIST&lt;/a&gt;
teria a seguinte arquitectura: 3 camadas escondidas, onde a primeira camada tem 300 neurónios, a segunda 200 e a terceira 100.
Esta prática tem sido abandonada porque a experiência mostra que usando o mesmo número de neurónios em todas as
camadas escondidas traz tão bons, ou melhores resultados, que o esquema em pirâmide. Traz ainda a vantagem de ficarmos
só com um hyperparâmetro para afinar em vez de um por camada. De qualquer forma e dependente do conjunto de dados que 
temos em mãos, às vezes ajuda fazer a primeira camada escondida maior do que as outras.
&lt;p&gt;
Tal como o número de camadas, podemos tentar aumentar o número de neurónios gradualmente até a rede começar a 
fazer &lt;i&gt;overfitting&lt;/i&gt;.Na prática é muito mais simples e eficiente escolher um modelo com mais camadas e 
mais neurónios do que o necessário e depois usar &lt;i&gt;early stopping&lt;/i&gt; e outras técnicas de regularização para prevenir o 
&lt;i&gt;overfitting.&lt;/i&gt;
&lt;/p&gt;
&lt;h3&gt;Taxa de aprendizagem&lt;/h3&gt;
Sem dúvida o hyperparâmetro mais importante. Uma boa forma de encontrar o melhor valor é treinar o modelo durante 
umas centenas de iterações , começando com um valor bastante baixo (e.g. \(10^{-5}\)) e gradualmente ir aumentando 
até um valor alto (e.g., 10). Acompanhar este exercício com uma visualização gráfica da função de perda em vs taxa
de aprendizagem.
&lt;h3&gt;Optimizador&lt;/h3&gt;
Há vários para além do clássico &lt;i&gt;Mini-batch Gradient Descent&lt;/i&gt; e fazer uma escolha cuidada é importante.
&lt;h3&gt;Tamanho do batch&lt;/h3&gt;
Esta escolha é muito dependente do hardware que temos em mãos para treinar a rede. Valores elevados
para o tamanho do batch requeremm aceleradores de hardware (GPUs).
&lt;h3&gt;Função de activação&lt;/h3&gt;
De uma forma geral, a função de activação ReLU é uma boa escolha para ser usada pelas camadas escondidas. Para a 
camada de saída (&lt;i&gt;output layer&lt;/i&gt;), depende da tarefa em mãos.</description>
        <pubDate>Thu, 25 Jun 2020 17:00:00 +0100</pubDate>
        <link>http://localhost:4000//FT_NN_Hyperparams</link>
        <link href="http://localhost:4000/FT_NN_Hyperparams"/>
        <guid isPermaLink="true">http://localhost:4000/FT_NN_Hyperparams</guid>
      </item>
    
      <item>
        <title>Regressão MLP com dados sintéticos</title>
        <description>&lt;script src=&quot;https://polyfill.io/v3/polyfill.min.js?features=es6&quot;&gt;&lt;/script&gt;
&lt;script id=&quot;MathJax-script&quot; async
          src=&quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&quot;&gt;
&lt;/script&gt;


&lt;h2&gt;Introdução&lt;/h2&gt;
Vamos analisar a capacidade de um MLP (Multi Layer Perceptron) em aproximar quatro funções diferentes.
&lt;br&gt;
Em cada caso vamos extrair &lt;i&gt;N = 50&lt;/i&gt; pontos escolhidos uniformemente em &lt;i&gt;x&lt;/i&gt; no intervalo (-1, 1) e os
correspondentes valores &lt;i&gt;f(x)&lt;/i&gt; calculados.
&lt;br&gt;
Estes pontos serão depois usados para treinar uma rede neuronal com:
&lt;ul&gt;
	&lt;li&gt;Duas camadas com:&lt;/li&gt;
	&lt;li&gt;Três unidades escondidas com:&lt;/li&gt;
	&lt;li&gt;Funcões de activação &lt;i&gt;tanh&lt;/i&gt; e:&lt;/li&gt;
	&lt;li&gt;Unidades de saída lineares&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
Nota: link para o jupyter notebook com o código completo: 
&lt;a href=&quot;https://github.com/mleiria/mleiria.github.io/blob/master/jupyter-notebook/MLP_Regression_Synthetic_Data.ipynb&quot; target=&quot;_blank&quot;&gt;MLP_Regression_Synthetic_Data.ipynb&lt;/a&gt;
&lt;/p&gt;

&lt;i&gt;Hyperparameters&lt;/i&gt; que podem ser alterados:
&lt;ul&gt;
	&lt;li&gt;Learning rate&lt;/li&gt;
	&lt;li&gt;Número de épocas&lt;/li&gt;
	&lt;li&gt;Tamanho do batch&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Módulos a importar&lt;/h3&gt;
&lt;pre&gt;
import pandas as pd
import tensorflow as tf
from matplotlib import pyplot as plt
&lt;/pre&gt;
&lt;h3&gt;Definir funções que constroem e treinam o modelo&lt;/h3&gt;
Vamos definir duas funções:
&lt;ul&gt;
	&lt;li&gt;&lt;pre&gt;build_model(my_learning_rate)&lt;/pre&gt; que constroi um modelo vazio&lt;/li&gt; 
	&lt;li&gt;&lt;pre&gt;train_model(model, feature, label, epochs)&lt;/pre&gt; que treina o modelo a partir dos exemplos (feature e label) que lhe passamos&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;
#@title Função para criar o modelo
def build_model(my_learning_rate):
    &quot;&quot;&quot;
    Cria e compila um modelo de regressão linear.
    
    Arguments:
    my_learning_rate -- a taxa de aprendizagem
    
    Returns:
    model -- o modelo compilado
    &quot;&quot;&quot;
    # O modelo mais simples de tf.keras é o sequencial
    # O modelo sequencial pode conter uma ou mais camadas
    model = tf.keras.models.Sequential()
    
    # Topografia do modelo
    # Duas camadas escondidas, cada uma com 3 unidades
    # A camada de output só tem uma unidade (visto que só queremos
    # prever um único valor) e não usa nenhuma função de activação
    model.add(tf.keras.layers.Dense(units=3, activation=&quot;tanh&quot;, input_shape=(1,)))
    model.add(tf.keras.layers.Dense(units=3, activation=&quot;tanh&quot;))
    model.add(tf.keras.layers.Dense(1))
    
    
    # Compilar a topografia do modelo
    # Configurar o treino para minimizar o erro quadrático médio
    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),
                  loss=&quot;mean_squared_error&quot;,
                  metrics=[tf.keras.metrics.RootMeanSquaredError()])
    
    return model           

#@title Função para treinar o modelo
def train_model(model, feature, label, epochs, batch_size):
    &quot;&quot;&quot;
    Treina o modelo de acordo com os dados de entrada
    
    Arguments:
    model -- o modelo a ser treinado
    feature -- um array de features (os valores x)
    label -- um array de labels (os valores y = f(x))
    epochs -- numero de epocas de treino
    batch_size -- tamanho do batch
    
    Returns:
    epochs -- Lista de épocas
    rmse -- Raíz quadrada do erro quadrático médio
    &quot;&quot;&quot;
    
    # Preparar uma diretoria de logs para ser usada pelo tensorboard
    log_dir = &quot;logs/fit/&quot; + datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)
    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

    # Passa os valores das features e os valores das labels
    # para o modelo. O modelo vai treinar durante o número
    # de epochs especificado e gradualmente vai aprendendo
    # como é que os valores das features se relacionam com
    # os valores das labels
    history = model.fit(x=feature,
                        y=label,
                        batch_size=batch_size,
                        epochs=epochs,
                        callbacks=[tensorboard_callback])
    
    # Gather the trained model's weight and bias.
    trained_weight = model.get_weights()
    trained_bias = model.get_weights()
    
    # A lista das epocas e guardada em separado
    epochs = history.epoch
    
    # Faz um snapshot do historico de cada epoca
    hist = pd.DataFrame(history.history)

    # Recolhe especificamente a raiz quadrada do erro quadrático médio
    # em cada epoca
    rmse = hist[&quot;root_mean_squared_error&quot;]

    return epochs, rmse

&lt;/pre&gt;
&lt;h3&gt;Definir funções auxiliares para visualizar os resultados&lt;/h3&gt;
Vamos definir duas funções de forma a conseguirmos visualizar o modelo
e a curva de perda:
&lt;ul&gt;
	&lt;li&gt;&lt;pre&gt;plot_the_model(feature, label, preds)&lt;/pre&gt; que mostra os pontos originais e uma linha com os pontos previstos pela rede&lt;/li&gt; 
	&lt;li&gt;&lt;pre&gt;plot_the_loss_curve(epochs, rmse)&lt;/pre&gt; que mostra a curva de perda em função das épocas&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;
#@title Gráfico para mostrar o modelo treinado
def plot_the_model(feature, label, preds):
    &quot;&quot;&quot;
    Gráfico que mostra o modelo treinado contra as features e labels
    
    Arguments:
    feature -- array de features que passámos à rede para treino
    label -- array de labels correspondentes às features
    preds -- array de valores previstos pelo modelo
    &quot;&quot;&quot;
    
    # Nomes para os eixos
    plt.xlabel(&quot;feature&quot;)
    plt.ylabel(&quot;label&quot;)
    
    # Valores das features vs. valores das labels
    plt.scatter(feature, label)
    
    # Cria uma representação a encarnado do modelo.
    plt.plot(feature, preds, 'r--')
    
    # Faz o render do scatter plot e da linha encarnada
    plt.show()

#@title Gráfico para mostrar o rmse vs. epochs
def plot_the_loss_curve(epochs, rmse):
    &quot;&quot;&quot;
    Gráfico da curva de perda (loss vs. epoch)
    
    Arguments:
    epochs -- número de épocas
    rms -- erro quadrático médio
    
    &quot;&quot;&quot;
    
    plt.figure()
    plt.xlabel(&quot;Epoch&quot;)
    plt.ylabel(&quot;Root Mean Squared Error&quot;)
    
    plt.plot(epochs, rmse, label=&quot;Loss&quot;)
    plt.legend()
    plt.ylim([rmse.min()*0.97, rmse.max()])
    plt.show()

&lt;/pre&gt;
&lt;h3&gt;Definir o conjunto de dados&lt;/h3&gt;
Vamos definir quatro funções para as quais queremos que a rede neuronal aprenda a sua representação:
&lt;ol&gt;
	&lt;li&gt;\(f(x) = x^2\)&lt;/li&gt;
	&lt;li&gt;\(f(x) = sin(x)\)&lt;/li&gt;
	&lt;li&gt;\(f(x) = |x|\)&lt;/li&gt;
	&lt;li&gt;\(f(x) = H(x)\)&lt;/li&gt;
&lt;/ol&gt;
onde \(H(x)\) é a função Heaviside.
&lt;pre&gt;
def draw_points(n, func):
    &quot;&quot;&quot;
    Constroi conjuntos de dados de acordo com a função escolhida
    
    Arguments:
    n -- Total de pontos
    func -- Tipo de função (Aceita: sin, power_two, abs, heaviside)
    
    Returns:
    feature -- Um array com as features (valores de x)
    label -- Um array com as labels (valores de y = f(x))
    &quot;&quot;&quot;
    feature = np.sort(np.random.uniform(-1, 1, n))
    if func == 'sin':
        label = np.sin(feature)
    elif func == 'power_two':
        label = np.power(feature, 2)
    elif func == 'abs':
        label = np.abs(feature)
    elif func == 'heaviside':
        label = np.heaviside(feature, 0)
    else:
        print(&quot;Erro&quot;)
        return 0, 0
        
    return feature, label    
&lt;/pre&gt;
Podemos agora gerar dados e ver a rede em acção:
&lt;pre&gt;
# Escolher uma das seguintes funções:
# my_feature, my_label = draw_points(50, func='power_two')
# my_feature, my_label = draw_points(50, func='sin')
# my_feature, my_label = draw_points(50, func='abs')

#Heaviside
my_feature, my_label = draw_points(50, func='heaviside')
&lt;/pre&gt;
&lt;h3&gt;Configurar os &lt;i&gt;Hyperparameters&lt;/i&gt; e treinar o modelo&lt;/h3&gt;
Aqui podemos experimentar diversas combinações dos &lt;i&gt;Hyperparameters&lt;/i&gt; para tentar obter o 
melhor ajustamento.

&lt;pre&gt;
# Limpa os logs das iterações anteriores
!rm -rf ./logs/ 
    
#Hyperparameters
learning_rate=0.01
epochs=100
my_batch_size=10

my_model = build_model(learning_rate)
epochs, rmse = train_model(my_model, my_feature, my_label, epochs, my_batch_size)	
&lt;/pre&gt;
&lt;h3&gt;Fazer previsões e visualizar os resultados.&lt;/h3&gt;
&lt;pre&gt;
# Fazer previsões
preds = my_model.predict(my_feature)
preds = np.squeeze(preds)

# Ver os resultados
plot_the_model(my_feature, my_label, preds)
plot_the_loss_curve(epochs, rmse)
&lt;/pre&gt;
Para este caso concreto da aproximação à função de Heaviside,
os parâmetros aprendidos pela rede neuronal é razoável:
&lt;img src=&quot;../images/2020-06-24-MLP_Regression_Sythetic_Data_2.png&quot;&gt;&lt;br&gt;
No gráfico de cima, Os pontos a azul representam os dados reais. Os traços a encarnado mostram
os outputs do modelo treinado. Idealmente a linha vermelha deve alinhar com os pontos azuis.
Há uma certa aleatoriedade quando o modelo é treinado, de forma que os resultados em cada treino
poderão ser ligeiramente diferentes.
O gráfico de baixo mostra a curva de perda. Podemos ver que a curva decresce, o que é bom, mas 
não fica plana, que é um indicativo que o modelo não treinou o suficiente.
Se alterarmos os &lt;i&gt;Hyperparameters&lt;/i&gt;:
&lt;pre&gt;
#Hyperparameters
learning_rate=0.01
epochs=200
my_batch_size=20
&lt;/pre&gt;
obtemos,
&lt;img src=&quot;../images/2020-06-24-MLP_Regression_Sythetic_Data_3.png&quot;&gt;
&lt;br&gt;
que melhora o ajuste.
&lt;h3&gt;Sumário aos ajustes dos &lt;i&gt;Hyperparameters&lt;/i&gt;.&lt;/h3&gt;
A maior parte dos problemas de machine learning requerem ajustes em vários parâmetros e o problema é que não há uma receita para cada modelo.&lt;br&gt;
Se baixarmos a taxa de aprendizagem (learning rate) podemos ajudar determinado modelo a convergir de forma eficiente mas pode fazer com que um outro modelo convirja lentamente. Há que experimentar
várias combinações do conjunto de &lt;i&gt;Hyperparameters&lt;/i&gt; de acordo com o conjunto de dados em estudo. Dito isto, há algumas &quot;regras&quot; para ajudar a escolher os &lt;i&gt;Hyperparameters&lt;/i&gt;:

&lt;ul&gt;
	&lt;li&gt;A função de perda do treino deve decrescer, primeiro de uma forma acentuada e depois mais lentamente até que o declive da curva se apróxima de zero.&lt;/li&gt;
	&lt;li&gt;Se a função de perda do treino não convergir =&gt; aumenta o número de épocas&lt;/li&gt;
	&lt;li&gt;Se a função de perda do treino decresce muito lentamente =&gt; aumenta a taxa de aprendizagem (learning rate). Atenção porque se a taxa for muito elevada
		pode fazer com que não haja convergência.&lt;/li&gt;
	&lt;li&gt;Se a função de perda do treino varia de forma descontrolada (altos e baixos acentuados) =&gt; diminui a taxa de aprendizagem&lt;/li&gt;
	&lt;li&gt;Diminuir a taxa de aprendizagem e aumentar o número de épocas ou o tamanho do batch, regra geral, é uma boa combinação.&lt;/li&gt;
	&lt;li&gt;Baixar muito o tamanho do batch pode causar alguma instabilidade. Primeiro tenta-se um valor elevado para o tamanho do batch e depois vai-se baixando 
		aos poucos.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Importante:&lt;/b&gt; A combinação ideal de &lt;i&gt;Hyperparameters&lt;/i&gt; é dependente do conjunto de dados em análise, de forma que devemos sempre experimentar
	várias combinações e verificar os resultados de forma a encontrar a combinação ideal.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Arquitectura típica de regressão MLP&lt;/h3&gt;
&lt;table&gt;
    &lt;tr&gt;&lt;th&gt;Hyperparameter&lt;/th&gt;&lt;th&gt;Valores típicos&lt;/th&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;# unidades de entrada (&lt;i&gt;input neurons&lt;/i&gt;)&lt;/td&gt;&lt;td&gt;Uma por feature de entrada&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;# camadas escondidas (&lt;i&gt;hidden layers&lt;/i&gt;)&lt;/td&gt;&lt;td&gt;Depende mas tipicamente de 1 a 5&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;# unidades por camada escondida (&lt;i&gt;neurons per hidden layer&lt;/i&gt;)&lt;/td&gt;&lt;td&gt;Depende mas tipicamente de 10 a 100&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;# unidades de saída (&lt;i&gt;output neurons&lt;/i&gt;)&lt;/td&gt;&lt;td&gt;1 por cada dimensão de previsão&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;Activação das camadas escondidas (&lt;i&gt;Hidden activation&lt;/i&gt;)&lt;/td&gt;&lt;td&gt;ReLU&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;Activação da saída (&lt;i&gt;output activation&lt;/i&gt;)&lt;/td&gt;&lt;td&gt;Nenhuma ou ReLU/softplus (se queremos outputs positivos) ou logistic/tanh (se queremos outputs limitados)&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td&gt;Função de perda (&lt;i&gt;loss function&lt;/i&gt;)&lt;/td&gt;&lt;td&gt;MSE ou MAE/Huber (se houver outliers)&lt;/td&gt;&lt;/tr&gt;

&lt;/table&gt;


</description>
        <pubDate>Thu, 25 Jun 2020 17:00:00 +0100</pubDate>
        <link>http://localhost:4000//Regressao-MLP-Dados-Sinteticos</link>
        <link href="http://localhost:4000/Regressao-MLP-Dados-Sinteticos"/>
        <guid isPermaLink="true">http://localhost:4000/Regressao-MLP-Dados-Sinteticos</guid>
      </item>
    
      <item>
        <title>Regressão Linear com TensorFlow. O essencial.</title>
        <description>&lt;script src=&quot;https://polyfill.io/v3/polyfill.min.js?features=es6&quot;&gt;&lt;/script&gt;
&lt;script id=&quot;MathJax-script&quot; async
          src=&quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&quot;&gt;
&lt;/script&gt;


&lt;h2&gt;Introdução&lt;/h2&gt;
Podemos definir um algoritmo de machine learning como um algoritmo que é capaz melhorar
a capacidade computacional de um programa a executar determinada tarefa através da experiência.&lt;br&gt;
Sendo esta definição um pouco abstracta, vamos apresentar um exemplo concreto: &lt;b&gt;regressão linear&lt;/b&gt;.&lt;br&gt;
Como o nome indica, o objectivo é construir um sistema que receba um vector \(\mathbf x \in R^n\) como input e prever
o valor de um escalar \(y \in R\) como output. O output da regressão linear é uma função linear do input.&lt;br&gt;
Seja \(\hat y\) o valor que o nosso modelo prevê que o valor real \(y\) deve tomar. Podemos definir o output
como:&lt;p&gt;
	\[\hat y = \mathbf w^T \mathbf x\]
&lt;/p&gt;
onde \(\mathbf w \in R^n\) é o vector de &lt;b&gt;parâmetros&lt;/b&gt;.&lt;br&gt;
Os parâmetros são os valores que controlam o comportamento do sistema. Neste caso, \(w_i\) é o coeficiente
que multiplicamos pelo valor de entrada (&lt;i&gt;feature&lt;/i&gt;) \(x_i\) antes de somarmos todas as contribuições
de todos os valores de entrada. Podemos pensar em \(\mathbf w\) como um conjunto de &lt;b&gt;pesos&lt;/b&gt; que determinam como
cada valor de entrada afecta a previsão.&lt;br&gt;
Se um valor de entrada \(x_i\) recebe um peso positivo \(w_i\), então, aumentando esse valor vai provocar um aumento 
no valor previsto \(\hat y\).&lt;br&gt;
Se um valor de entrada \(x_i\) recebe um peso negativo \(w_i\), então, aumentando esse valor vai provocar um decréscimo 
no valor previsto \(\hat y\).&lt;p&gt;
Desta forma temos a definição da nossa tarefa &lt;i&gt;T&lt;/i&gt;: prever \(y\) a partir de \(\mathbf x\) usando \(\hat y = \mathbf w^T\mathbf x\).&lt;br&gt;
O próximo passo é deifinir uma medida de &lt;i&gt;performance&lt;/i&gt; &lt;i&gt;P&lt;/i&gt;.
&lt;/p&gt;
Uma das formas de medir esta &lt;i&gt;performance&lt;/i&gt; é calculando o &lt;b&gt;erro quadtrático médio&lt;/b&gt; do modelo no conjunto de dados de teste. Se
\( \mathbf{ \hat y}^{(test)} \) dá-nos a previsão do modelo no conjunto de dados de teste, então o erro quadtrático médio é dado por:&lt;p&gt;

	\[ MSE_{test} = \frac{1}{m} \sum_i (\mathbf{ \hat y}^{(test)} - \mathbf{y}^{(test)})^2_i\]
&lt;br&gt;
ou, alternativamente
&lt;br&gt;
	\[ MSE_{test} = \frac{1}{m} \| \mathbf{ \hat y}^{(test)} - \mathbf{y}^{(test)}\|^2_2\]
&lt;/p&gt;
Intuitivamente podemos ver que esta medida de erro decresce para 0 quando \( \mathbf{ \hat y}^{(test)} = \mathbf{ y}^{(test)} \)
&lt;h2&gt;Procedimento&lt;/h2&gt;
Para fazermos um algoritmo de machine learning, temos de desenhar um algoritmo que melhore os pesos \( \mathbf w\) de uma maneira que reduza 
o \( MSE_{test} \) quando é permitido ao algoritmo ganhar experiência através da observação do conjunto de dados de treino \( (\mathbf X^{(train)}, \mathbf Y^{(train)}) \).&lt;br&gt;
A forma mais natural de minimizar este erro é obter o gradiente e igualá-o a zero:&lt;p&gt;

	\[ \nabla_w MSE_{train} = 0 \]
	\[ \Rightarrow \nabla_w \frac{1}{m} \| \mathbf{ \hat y}^{(train)} - \mathbf{y}^{(train)}\|^2_2 = 0\]
	\[ \Rightarrow \frac{1}{m} \nabla_w  \| \mathbf X^{(train)} \mathbf w - \mathbf{y}^{(train)}\|^2_2 = 0\]
	\[ \Rightarrow \mathbf w = (\mathbf X^{(train)T} \mathbf X^{(train)})^{-1} \mathbf X^{(train)T} \mathbf y^{(train)} \]
&lt;/p&gt; 
Vale a pena referir que o termo &lt;b&gt;regressão linear&lt;/b&gt; é usado exprimir um modelo ligeiramente mais sofisticado, com um parâmetro adicional - 
um termo de intersecção \(b\):&lt;p&gt;
	\[\hat y = \mathbf w^T \mathbf x + b\]
&lt;/p&gt;
&lt;h2&gt;TensorFlow&lt;/h2&gt;
Tentando manter as coisas simples, vamos usar TensorFlow para converter graus Celsius para Fahrenheit. A fórmula aproximada é:&lt;p&gt;
	\[ f = 1.8c + 32\]
&lt;/p&gt;

o exercício será passarmos ao TensorFlow uma amostra de dados Celcius e os seus correspondentes Fahrenheit. Depois, treinamos um modelo que
replique a fórmua supra de conversão, através de um processo de treino.&lt;br&gt;
&lt;br&gt;
&lt;h4&gt;1 - Importar dependências&lt;/h4&gt;

&lt;pre&gt;
import tensorflow as tf
import numpy as np
&lt;/pre&gt;

&lt;br&gt;
&lt;h4&gt;2 - Preparar o conjunto de dados de treino&lt;/h4&gt;
Este é um algoritmo de machine learning supervisionado porque vamos mostrar ao modelo alguns exemplos de conversão e queremos que ele nos
dê uma fórmula generalizada:&lt;br&gt;

&lt;pre&gt;
celsius_q    = np.array([-40, -10,  0,  8, 15, 22,  38],  dtype=float)
fahrenheit_a = np.array([-40,  14, 32, 46, 59, 72, 100],  dtype=float)
&lt;/pre&gt;

NOTA: Alguma terminologia para o que se segue:&lt;p&gt;
&lt;ul&gt;
	&lt;li&gt;
&lt;b&gt;Feature&lt;/b&gt; - os valores de input para o nosso modelo. Neste caso é um só valor: os graus em Celcius.
&lt;/li&gt;
&lt;li&gt;
&lt;b&gt;Labels&lt;/b&gt; - o output que o nosso modelo vai prever. Neste caso é um só valor: os graus em Fahrenheit.
&lt;/li&gt;
&lt;li&gt;
&lt;b&gt;Exemplo&lt;/b&gt; - um par de valores input/output usados na fase de treino. Neste caso é um par de valores (Celcius, Fahrenheit)
&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;h4&gt;3 - Criar o modelo&lt;/h4&gt;
O passo seguinte é criar o modelo. Como este é um problema muito simples basta-nos criar uma rede com uma única camada
e um único nó.
&lt;br&gt;

&lt;h4&gt;3.1 - Construir a camada (&lt;i&gt;layer&lt;/i&gt;)&lt;/h4&gt;

&lt;pre&gt;l0 = tf.keras.layers.Dense(units=1, input_shape=[1])&lt;/pre&gt;
onde
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;l0&lt;/i&gt;: Nome da camada&lt;/li&gt;
&lt;li&gt;&lt;i&gt;input_shape=[1]&lt;/i&gt;: Especifica que o input desta camada é um único valor, ou seja, a forma é um array de uma dimensão com um membro.
O valor é do tipo float e representa os graus Celcius&lt;/li&gt;
&lt;li&gt;&lt;i&gt;units=1&lt;/i&gt;:Especifica o número de neurónios (ou nós, ou unidades) na camada. O número de neurónios define quantas variáveis internas 
a camada tem para tentar resolver o problema&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h4&gt;3.2 - Adicionar a camada ao modelo&lt;/h4&gt;
Uma vez definidas as camadas, podemos adicioná-las ao modelo. O modelo sequencial recebe uma lista de camadas como argumento que especifica 
a ordem de cálculo desde o input até ao output. No nosso caso só temos uma camada:
&lt;pre&gt;
model = tf.keras.Sequential([l0])
&lt;/pre&gt;
&lt;br&gt;
&lt;h4&gt;4 - Compilar o modelo com as funções de perda e otimizador&lt;/h4&gt;
Antes de treinar o modelo, é necessário compilar, definindo duas funções importantes:
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Função de perda (&lt;i&gt;loss function&lt;/i&gt;)&lt;/b&gt;: uma forma de medir o quão afastado está o valor previsto pela rede do valor real. Neste caso
faz sentido usar o erro quadrático médio.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Função otimizadora&lt;/b&gt;: uma forma de ajustar os valores internos de forma a reduzir a perda.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.1))&lt;/pre&gt;
Estas funções serão utilizadas durante a fase de treino (&lt;i&gt;model.fit()&lt;/i&gt;), primeiro para calcular a perda em cada ponto e posteriormente
melhorá-la.
&lt;br&gt;
Note-se que o otimizador Adam, recebe um argumento, o &lt;i&gt;learning rate&lt;/i&gt;. Este é um hyperparâmetro e representa o comprimento do passo a ser dado, em cada
iteração, quando o modelo ajusta os seus valores. Normalmente escolhemos um valor entre 0.001 e 0.1.

&lt;br&gt;&lt;br&gt;
&lt;h4&gt;5 - Treinar o modelo&lt;/h4&gt;
O treino do modelo é feito invocando o método &lt;i&gt;fit()&lt;/i&gt;.&lt;br&gt;
Durante o treino o modelo vai receber os valores em graus Celcius, fazer algumas contas usando os pesos e retorna como output o que ele considera
ser a conversão para Fahrenheit. Como os pesos vão ser inicializados com valore aleatórios, o primeiro output não vai ter nada a ver com o valor
correcto da conversão. Depois é utilizada a função de perda para calcular o quão afastado o outuput está do valor real. Por fim a função de otimização
reajusta os pesos de forma a aproximar o output do valor real.
&lt;p&gt;
Este ciclo de calcular, comparar e ajustar é controlado pelo ḿétodo &lt;i&gt;fit()&lt;/i&gt;: o primeiro argumento são os inputs (graus Celcius), o segundo argumento
são os outputs desejados (graus Fahrenheit). O argumento &lt;i&gt;epochs&lt;/i&gt; especifica quantas vezes este ciclo deve ser executado
&lt;/p&gt;
&lt;pre&gt;history = model.fit(celsius_q, fahrenheit_a, epochs=500, verbose=False)&lt;/pre&gt;
&lt;br&gt;
&lt;h4&gt;6 - Mostrar as estatísticas do treino&lt;/h4&gt;
O método &lt;i&gt;fit()&lt;/i&gt; retorna um objecto de histórico. Podemos usar este objecto para analisar como é que a perda evolui ao longo das épocas. 
&lt;br&gt;
Uma perda elevada diz-nos que a conversão para Fahrenheit prevista pelo nosso modelo está longe do valor verdadeiro. Vejamos o aspeto da 
curva de perda:
&lt;pre&gt;
import matplotlib.pyplot as plt
plt.xlabel('Epoch')
plt.ylabel(&quot;Root Mean Squared Error&quot;)
plt.plot(history.history['loss'])
&lt;/pre&gt;
&lt;img src=&quot;../images/2020-06-04-LossCurve.png&quot; alt=&quot;Loss curve&quot;&gt;
&lt;br&gt;&lt;br&gt;
&lt;h4&gt;6 - Usar o modelo para fazer previsões&lt;/h4&gt;
Com o modelo treinado, estamos em condições de fazer previsões, i.e., apresentar um determinado valor (graus Celcius) à rede neuronal e obter
o respetivo valor em Fahrenheit.&lt;br&gt;
Por exemplo, 100º Celcius corresponde a quantos graus Fahrenheit? Note-se que este valor não foi apresentado à rede na fase de treino.
&lt;pre&gt;print(model.predict([100.0]))&lt;/pre&gt;
[[211.75616]]
&lt;br&gt;&lt;br&gt;
A resposta correta (utilizando a fórmula de conversão), é:&lt;p&gt;
	100×1.8+32=212
&lt;/p&gt;
Não está nada mal!
&lt;br&gt;&lt;br&gt;
Resumindo:
&lt;ul&gt;
&lt;li&gt;Criar o modelo&lt;/li&gt;
&lt;li&gt;Treinar o modelo com 35000 exemplos (7 pares em 500 épocas)&lt;/li&gt;
&lt;/ul&gt;
O nosso modelo foi afinando os pesos na camada densa até conseguir retornar o valor correto em Fahrenheit para qualquer valor Celcius.
&lt;br&gt;&lt;br&gt;
Por último podemos ver os valores finais dos pesos determinados pela rede:
&lt;pre&gt;print(&quot;These are the layer variables: {}&quot;.format(l0.get_weights()))&lt;/pre&gt;

[array([[1.7974412]], dtype=float32), array([31.949804], dtype=float32)]
&lt;br&gt;&lt;br&gt;
A primeira variável está próxima de \(~1.8\) e a segunda, próxima de \(~32\). Estes valores (\(1.8\) e \(32\)) são os valores da fórmula 
de conversão.
&lt;p&gt;
	O código completo está aqui: &lt;a href=&quot;https://github.com/mleiria/mleiria.github.io/blob/master/jupyter-notebook/Linear_Regression_Tensorflow.ipynb&quot; target=&quot;_blank&quot;&gt;Linear_Regression_Tensorflow.ipynb&lt;/a&gt;
	
&lt;/p&gt;



</description>
        <pubDate>Thu, 04 Jun 2020 17:50:00 +0100</pubDate>
        <link>http://localhost:4000//Linear-Regression-Tensorflow</link>
        <link href="http://localhost:4000/Linear-Regression-Tensorflow"/>
        <guid isPermaLink="true">http://localhost:4000/Linear-Regression-Tensorflow</guid>
      </item>
    
      <item>
        <title>Álgebra Linear. O essencial.</title>
        <description>&lt;script src=&quot;https://polyfill.io/v3/polyfill.min.js?features=es6&quot;&gt;&lt;/script&gt;
&lt;script id=&quot;MathJax-script&quot; async
          src=&quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&quot;&gt;
&lt;/script&gt;

&lt;h2&gt;Multiplicação de Matrizes e Vectores&lt;/h2&gt;
&lt;p&gt;Uma das mais importantes operações que envolvem matrizes é a multiplicação de duas matrizes.&lt;/p&gt;
O &lt;b&gt;produto matricial&lt;/b&gt; entre duas matrizes \(A\) e \(B\) é uma terceira matriz \(C\).&lt;br&gt;
Para este produto ser definido, a matriz \(A\) tem de ter o mesmo número de colunas que o número de linhas
da matriz \(B\).&lt;br&gt;
Se \(A\) tiver dimensões (\(m\) x \(n\)), onde \(m\) representa o número de linhas e \(m\) é o número de colunas e \(B\) tiver dimensões 
\((n\) x \(p\)), então \(C\) terá dimensões (\(m\) x \(p\)).&lt;br&gt;
Por exemplo:&lt;p&gt;
	
	\[C = AB\]
&lt;/p&gt;
Esta operação é definida da seguinte forma:&lt;p&gt;

	\[C_{i,j} = \sum_{k} A_{i,k} B_{k,j}\]
&lt;/p&gt;
Atenção que este produto não corresponde ao produto dos elementos individuais. Este produto também existe e chama-se &lt;b&gt;produto Hadamard&lt;/b&gt; ou 
&lt;b&gt;produto elemento a elemento &lt;/b&gt;(&lt;i&gt;element-wise&lt;/i&gt;) e denota-se 
por \(A \odot B\)
&lt;p&gt;
O &lt;b&gt;produto vectorial&lt;/b&gt; (&lt;i&gt;dot product&lt;/i&gt;) entre dois vectores \(x\) e \(y\) com a mesma dimensão, é o produto matricial \(x^T y\)&lt;br&gt;
O produto matricial apresenta algumas propriedades úteis. Por exemplo a distributividade:&lt;p&gt;
	\[A(B + C) = AB + AC\]
&lt;/p&gt;Também é associativo:&lt;p&gt;
	\[A(BC) = (AB)C\]
&lt;/p&gt;
A multiplicação matricial &lt;i&gt;não é comutativa&lt;/i&gt; (a condição \(AB = BA\) nem sempre é válida). No entanto o produto vectorial entre dois
vectores é comutativa:&lt;p&gt;
	\[x^Ty = y^Tx\]
&lt;/p&gt;
A matriz transposta do produto de duas matrizes é:&lt;p&gt;
	\[(AB)^T = B^TA^T\]
&lt;/p&gt;
&lt;h2&gt;Identidade e Matriz Inversa&lt;/h2&gt;
Para descrever a matriz inversa, precisamos primeiro definir o conceito de &lt;b&gt;matriz identidade&lt;/b&gt;. Uma matriz identidade é uma matriz que
não altera qualquer vector que for multiplicado por ela. Formalmente, \(I_{n} \in R^{nxn}\) e &lt;p&gt;
	\[\forall x \in R^{n}, I_nx = x\]
&lt;/p&gt;
A estrutura da matriz identidade é simples: todas as entradas da diagonal principal são 1 e todas as outras são 0. Por exemplo:&lt;p&gt;
	\[I_3 = \begin{bmatrix}1 &amp; 0 &amp; 0\\0 &amp; 1 &amp; 0 \\0 &amp; 0&amp; 1\end{bmatrix}\]
&lt;/p&gt;
A &lt;b&gt;matriz inversa&lt;/b&gt; de \(A\), denota-se por \(A^{-1}\) e é difinida como a matriz tal que:&lt;p&gt;
	\[A^{-1} A =I_n\]
&lt;/p&gt;
Consideremos como exercício final o seguinte sistema de equações lineares:&lt;p&gt;
	\[A x = b\]
&lt;/p&gt;
onde \(A \in R^{mxn}\) é uma matriz com os valores conhecidos, \(b \in R^m\) é um vector conhecido e \(x \in R^{n}\) é o vector de variáveis 
desconhecidas para o qual pretendemos resolver o sistema. Esta equação resolve-se seguindo os seguintes passos:&lt;p&gt;
	\[A x = b\]
	\[A^{-1}A x = A^{-1}b\]
	\[I_n x = A^{-1}b\]
	\[x = A^{-1}b\]
&lt;/p&gt;
&lt;h2&gt;Norma&lt;/h2&gt;
Em &lt;i&gt;machine learning&lt;/i&gt; normalmente mede-se o comprimento de um vector usando uma função chamada &lt;b&gt;norma&lt;/b&gt;. Formalmente,
a norma \(L^p\) é dada por:&lt;p&gt;
	\[||x||_p = (\sum_i |x_i|^p)^{1/p}\]
para \(p \in R, p \geq 1\)
&lt;/p&gt;
As normas são funções que mapeiam vectores para valores não negativos. Intiuitivamente podemos pensar que a norma de um vector \(x\) 
mede a distância da origem ao ponto \(x\). Sendo um pouco mais rigorosos, a norma é qualquer função \(f\) que satisfaz as seguintes propriedades:&lt;p&gt;
	&lt;ul&gt;
		&lt;li&gt;\(f(x) = 0\ \Rightarrow x = 0\)&lt;/li&gt;
		&lt;li&gt;\(f(x + y) \leq f(x) + f(y)\)&lt;/li&gt;
		&lt;li&gt;\(\forall \alpha \in R, f(\alpha x) = |\alpha|f(x)\)&lt;/li&gt;
	&lt;/ul&gt;
&lt;/p&gt;
A norma \(L^2\) (quando \(p = 2\)) é conhecida como a &lt;b&gt;norma Euclideana&lt;/b&gt;, que é simplesmente a distância euclideana da origem ao ponto 
identificado por \(x\). Costuma-se designar por \(||x||\), onde o \(2\) é omitido na notação.&lt;br&gt;
Também é bastante comum medir o comprimento de um vector usando o quadrado da norma \(L^2\), que pode ser calculado simplesmente como
\(x^Tx\)&lt;br&gt;
A norma \(L^1\) também é bastante utilizada:&lt;p&gt;
	\[||x||_1 = \sum_i |x_i|\]
&lt;/p&gt;
Outra norma bastante utilizada nesta área é a norma \(L^\infty\), também conhecida como &lt;b&gt;norma máxima&lt;/b&gt; (&lt;i&gt;max norm&lt;/i&gt;):&lt;p&gt;
	\[||x||_\infty = \mathrm{max}_i|x_i|\]
&lt;/p&gt;
Por fim, muitas vezes é necessário calcular o comprimento de uma matriz. No contexto de &lt;i&gt;deep learning&lt;/i&gt; a norma mais utilizada 
é a &lt;b&gt;norma Frobenius&lt;/b&gt;:&lt;p&gt;
	\[||A||_F = \sqrt{\sum_{i,j} A^2_{i,j}}\]
&lt;/p&gt;
que é semelhante à norma \(L^2\) de um vector.&lt;br&gt;
	O produto vectorial de dois vectores pode ser escrito em termos das suas normas:&lt;p&gt;
		\[x^Ty = ||x||_2 ||y||_2 cos \theta\]
&lt;/p&gt;
onde \(\theta\) é o ângulo entre \(x\) e \(y\)
&lt;h2&gt;Matrizes e vectores especiais&lt;/h2&gt;
&lt;h3&gt;Matriz diagonal&lt;/h3&gt;
Esta matriz só apresenta valores diferentes de zero na sua diagonal principal. Formalmente, uma matriz \(D\) é diagonal se e só se \(D_{i,j} = 0\) para todos
os \(i \neq j\). Já vimos um exemplo de uma matriz diagonal: a matriz identidade, onde todas as entradas na diagonal principal são 1.
&lt;h3&gt;Matriz simétrica&lt;/h3&gt;
É qualquer matriz tal que é igual à sua transposta:&lt;p&gt;
	\[A = A^T\]
&lt;/p&gt;
&lt;h3&gt;Vector unitário&lt;/h3&gt;
É um vector com a &lt;b&gt;norma unitária&lt;/b&gt;:&lt;p&gt;
	\[||x||_2 = 1\]
&lt;/p&gt;
Um vector \(x\) e um vector \(y\) são ortogonais entre si, se \(x^Ty = 0\). Se ambos os vectores tiverem uma norma diferente de zero, isto quer dizer que formam um 
ângulo de \(90^0\) entre si. Se dois vectores forem ortogonais e tiverem norma unitária, chamam-se &lt;b&gt;ortonormais&lt;/b&gt;.
&lt;h3&gt;Matriz ortogonal&lt;/h3&gt;
É uma matriz quadrada cujas linhas são mutualmente ortonormais e cujas colunas são mutualmente ortonormais:&lt;p&gt;
	\[A^TA = AA^T = I\]
&lt;/p&gt;
Isto implica que:&lt;p&gt;
	\[A^{-1} = A^T\]
&lt;/p&gt;
Estas matrizes são interessantes porque a sua inversa é computacionalmente fácil de obter.
&lt;p&gt;
&lt;script src=&quot;https://gist.github.com/mleiria/4f27f0361a01d8485e103b2a73ae8080.js&quot;&gt;&lt;/script&gt;
&lt;/p&gt;
</description>
        <pubDate>Thu, 04 Jun 2020 17:50:00 +0100</pubDate>
        <link>http://localhost:4000//Algebra-Linear</link>
        <link href="http://localhost:4000/Algebra-Linear"/>
        <guid isPermaLink="true">http://localhost:4000/Algebra-Linear</guid>
      </item>
    
  </channel>
</rss>
